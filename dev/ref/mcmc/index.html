<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Markov chain Monte Carlo · Gen</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link href="../../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>Gen</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../../">Home</a></li><li><a class="toctext" href="../../getting_started/">Getting Started</a></li><li><a class="toctext" href="../../tutorials/">Tutorials</a></li><li><span class="toctext">Modeling Languages and APIs</span><ul><li><a class="toctext" href="../gfi/">Generative Functions</a></li><li><a class="toctext" href="../distributions/">Probability Distributions</a></li><li><a class="toctext" href="../modeling/">Built-in Modeling Language</a></li><li><a class="toctext" href="../combinators/">Generative Function Combinators</a></li><li><a class="toctext" href="../choice_maps/">Choice Maps</a></li><li><a class="toctext" href="../selections/">Selections</a></li><li><a class="toctext" href="../parameter_optimization/">Optimizing Trainable Parameters</a></li><li><a class="toctext" href="../extending/">Extending Gen</a></li></ul></li><li><span class="toctext">Standard Inference Library</span><ul><li><a class="toctext" href="../importance/">Importance Sampling</a></li><li class="current"><a class="toctext" href>Markov chain Monte Carlo</a><ul class="internal"><li><a class="toctext" href="#MCMC-in-Gen-1">MCMC in Gen</a></li><li><a class="toctext" href="#Built-in-Stationary-Kernels-1">Built-in Stationary Kernels</a></li><li><a class="toctext" href="#Enabling-Dynamic-Checks-1">Enabling Dynamic Checks</a></li><li><a class="toctext" href="#Composite-Kernel-DSL-1">Composite Kernel DSL</a></li><li><a class="toctext" href="#Reverse-Kernels-1">Reverse Kernels</a></li><li><a class="toctext" href="#API-1">API</a></li></ul></li><li><a class="toctext" href="../map/">MAP Optimization</a></li><li><a class="toctext" href="../pf/">Particle Filtering</a></li><li><a class="toctext" href="../vi/">Variational Inference</a></li><li><a class="toctext" href="../learning/">Learning Generative Functions</a></li></ul></li><li><span class="toctext">Internals</span><ul><li><a class="toctext" href="../internals/parameter_optimization/">Optimizing Trainable Parameters</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li>Standard Inference Library</li><li><a href>Markov chain Monte Carlo</a></li></ul><a class="edit-page" href="https://github.com/probcomp/Gen/blob/master/docs/src/ref/mcmc.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Markov chain Monte Carlo</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Markov-chain-Monte-Carlo-(MCMC)-1" href="#Markov-chain-Monte-Carlo-(MCMC)-1">Markov chain Monte Carlo (MCMC)</a></h1><p>Markov chain Monte Carlo (MCMC) is an approach to inference which involves initializing a hypothesis and then repeatedly sampling a new hypotheses given the previous hypothesis by making a change to the previous hypothesis. The function that samples the new hypothesis given the previous hypothesis is called the <strong>MCMC kernel</strong> (or `kernel&#39; for short). If we design the kernel appropriately, then the distribution of the hypotheses will converge to the conditional (i.e. posterior) distribution as we increase the number of times we apply the kernel.</p><p>Gen includes primitives for constructing MCMC kernels and composing them into MCMC algorithms. Although Gen encourages you to write MCMC algorithms that converge to the conditional distribution, Gen does not enforce this requirement. You may use Gen&#39;s MCMC primitives in other ways, including for stochastic optimization.</p><p>For background on MCMC see [1].</p><p>[1] Andrieu, Christophe, et al. &quot;An introduction to MCMC for machine learning.&quot; Machine learning 50.1-2 (2003): 5-43. <a href="https://www.cs.ubc.ca/~arnaud/andrieu_defreitas_doucet_jordan_intromontecarlomachinelearning.pdf">Link</a>.</p><h2><a class="nav-anchor" id="MCMC-in-Gen-1" href="#MCMC-in-Gen-1">MCMC in Gen</a></h2><p>Suppose we are doing inference in the following toy model:</p><pre><code class="language-julia">@gen function model()
    x = @trace(bernoulli(0.5), :x) # a latent variable
    @trace(normal(x ? -1. : 1., 1.), :y) # the variable that will be observed
end</code></pre><p>To do MCMC, we first need to obtain an initial trace of the model. Recall that a trace encodes both the observed data and hypothesized values of latent variables. We can obtain an initial trace that encodes the observed data, and contains a randomly initialized hypothesis, using <a href="../gfi/#Gen.generate"><code>generate</code></a>, e.g.:</p><pre><code class="language-julia">observations = choicemap((:y, 1.23))
trace, = generate(model, (), observations)</code></pre><p>Then, an MCMC algorithm is Gen is implemented simply by writing Julia <code>for</code> loop, which repeatedly applies a kernel, which is a regular Julia function:</p><pre><code class="language-julia">for i=1:100
    trace = kernel(trace)
end</code></pre><h2><a class="nav-anchor" id="Built-in-Stationary-Kernels-1" href="#Built-in-Stationary-Kernels-1">Built-in Stationary Kernels</a></h2><p>However, we don&#39;t expect to be able to use any function for <code>kernel</code> and expect to converge to the conditional distribution. To converge to the conditional distribution, the kernels must satisfy some properties. One of these properties is that the kernel is <strong>stationary</strong> with respect to the conditional distribution. Gen&#39;s inference library contains a number of functions for constructing stationary kernels:</p><ul><li><p><a href="#Gen.metropolis_hastings"><code>metropolis_hastings</code></a> with alias <a href="#Gen.mh"><code>mh</code></a>, which has three variants with differing tradeoffs between ease-of-use and efficiency. The simplest variant simply requires you to select the set of random choices to be updated, without specifying how. The middle variant allows you to use custom proposals that encode problem-specific heuristics, or custom proposals based on neural networks that are trained via amortized inference. The most sophisticated variant allows you to specify any kernel in the <a href="https://people.maths.bris.ac.uk/~mapjg/papers/RJMCMCBka.pdf">reversible jump MCMC</a> framework.</p></li><li><p><a href="#Gen.mala"><code>mala</code></a>, which performs a Metropolis Adjusted Langevin algorithm update on a set of selected random choices.</p></li><li><p><a href="#Gen.hmc"><code>hmc</code></a>, which performs a Hamiltonian Monte Carlo update on a set of selected random choices.</p></li><li><p><a href="#Gen.elliptical_slice"><code>elliptical_slice</code></a>, which performs an elliptical slice sampling update on a selected multivariate normal random choice.</p></li></ul><p>For example, here is an MCMC inference algorithm that uses <a href="#Gen.mh"><code>mh</code></a>:</p><pre><code class="language-julia">function do_inference(y, num_iters)
    trace, = generate(model, (), choicemap((:y, y)))
    xs = Float64[]
    for i=1:num_iters
        trace, = mh(trace, select(:x))
        push!(xs, trace[:x])
    end
    xs
end</code></pre><p>Note that each of the kernel functions listed above stationary with respect to the joint distribution on traces of the model, but may not be stationary with respect to the intended conditional distribution, which is determined by the set of addresses that consititute the observed data. If a kernel modifies the values of any of the observed data, then the kernel is not stationary with respect to the conditional distribution. Therefore, you should <strong>ensure that your MCMC kernels never propose to the addresses of the observations</strong>.</p><p>Note that stationarity with respect to the conditional distribution alone is not sufficient for a kernel to converge to the posterior with infinite iterations. Other requirements include that the chain is <strong>irreducible</strong> (it is possible to get from any state to any other state in a finite number of steps), and <strong>aperiodicity</strong>, which is a more complex requirement that is satisfied when kernels have some probability of staying in the same state, which most of the primitive kernels above satisfy. We refer interested readers to [1] for additional details on MCMC convergence.</p><h2><a class="nav-anchor" id="Enabling-Dynamic-Checks-1" href="#Enabling-Dynamic-Checks-1">Enabling Dynamic Checks</a></h2><p>Gen does not statically guarantee that kernels (either ones built-in or composed with the <a href="#Composite-Kernel-DSL-1">Composite Kernel DSL</a>) are stationary. However, you can enable dynamic checks that will detect common bugs that break stationarity. To enable the dynamic checks we pass a keyword argument beyond those of the kernel itself:</p><pre><code class="language-julia">new_trace = k(trace, 2, check=true)</code></pre><p>Note that these checks aim to detect when a kernel is not stationary with respect to the model&#39;s <strong>joint</strong> distribution. To add an additional dynamic check for violation of stationarity with respect to the <em>conditional</em> distribution (conditioned on observations), we pass in an additional keyword argument containing a choice map with the observations:</p><pre><code class="language-julia">new_trace = k(traced, 2, check=true, observations=choicemap((:y, 1.2)))</code></pre><p>If <code>check</code> is set to <code>false</code>, then the observation check is not performed.</p><h2><a class="nav-anchor" id="Composite-Kernel-DSL-1" href="#Composite-Kernel-DSL-1">Composite Kernel DSL</a></h2><p>You can freely compose the primitive kernels listed above into more complex kernels. Common types of composition including e.g. cycling through multiple kernels, randomly choosing a kernel to apply, and choosing which kernel to apply based on the current state. However, not all such compositions of stationary kernels will result in kernels that are themselves stationary.</p><p>Gen&#39;s <strong>Composite Kernel DSL</strong> is an embedded inference DSL that allows for more safe composition of MCMC kernels, by formalizing properties of the compositions that are sufficient for stationarity, encouraging compositions with these properties, and dynamically checking for violation of these properties. Although the DSL does not <em>guarantee</em> stationarity of the composite kernels, its dynamic checks do catch common cases of non-stationary kernels. The dynamic checks can be enabled and disabled as needed (e.g. enabled during testing and prototyping and disabled during deployment for higher performance).</p><p>The DSL consists of a macro – <a href="#Gen.@kern"><code>@kern</code></a> for composing stationary kernels from primitive stationary kernels and composite stationary kernels, and two additional macros: –- <a href="#Gen.@pkern"><code>@pkern</code></a> for declaring Julia functions to be custom primitive stationary kernels, and <a href="#Gen.@rkern"><code>@rkern</code></a> for declaring the reversal of a custom primitive kernel (these two macros are advanced features not necessary for standard MCMC algorithms).</p><h3><a class="nav-anchor" id="Composing-Stationary-Kernels-1" href="#Composing-Stationary-Kernels-1">Composing Stationary Kernels</a></h3><p>The <a href="#Gen.@kern"><code>@kern</code></a> macro defines a composite MCMC kernel in a restricted DSL that is based on Julia&#39;s own function definition syntax.</p><p>Suppose we are doing inference in the following model:</p><pre><code class="language-julia">@gen function model()
    n = @trace(geometric(0.5), :n)
    total = 0.
    for i=1:n
        total += @trace(normal(0, 1), (:x, i))
    end
    @trace(normal(total, 1.), :y)
    total
end</code></pre><p>Here is an example composite kernel for MCMC in this model:</p><pre><code class="language-julia">@kern function my_kernel(trace)
    
    # cycle through the x&#39;s and do a random walk update on each one
    for i in 1:trace[:n]
        trace ~ mh(trace, random_walk_proposal, (i,))
    end

    # repeatedly pick a random x and do a random walk update on it
    if trace[:n] &gt; 0
        for rep in 1:10
            let i ~ uniform_discrete(1, trace[:n])
                trace ~ mh(trace, random_walk_proposal, (i,))
            end
        end
    end

    # remove the last x, or add a new one, a random number of times
    let n_add_remove_reps ~ uniform_discrete(0, max_n_add_remove)
        for rep in 1:n_add_remove_reps
            trace ~ mh(trace, add_remove_proposal, (), add_remove_involution)
        end
    end
end</code></pre><p>In the DSL, the first arugment (<code>trace</code> in this case) represents the trace on which the kernel is acting. the kernel may have additional arguments. The code inside the body can read from the trace (e.g. <code>trace[:n]</code> reads the value of the random choice <code>:n</code>). Finally, the return value of the composite kernel is automatically set to the trace. NOTE: It is not permitted to assign to the trace variable, except with <code>~</code> expressions. Also note that stationary kernels, when treated as Julia functions, return a tuple, where the first element is the trace and the remaining arguments are metadata. When applying these kernels with <code>~</code> syntax within the DSL, it is not necessary to unpack the tuple (the metadata is ignored automatically).</p><p>The language constructs supported by this DSL are:</p><p><strong>Applying a stationary kernel.</strong> To apply a kernel, the syntax <code>trace ~ k(trace, args..)</code> is used. Note that the <code>check</code> and <code>observations</code> keyword arguments (see <a href="#Enabling-Dynamic-Checks-1">Enabling Dynamic Checks</a>) should not be used here; they will be added automatically.</p><p><strong>For loops.</strong> The range of the for loop may be a deterministic function of the trace (as in <code>trace[:n]</code> above). The range must be <em>invariant</em> under all possible executions of the body of the for loop. For example, the random walk based kernel embedded in the for loop in our example above cannot modify the value of the random choice <code>:n</code> in the trace.</p><p><strong>If-end expressions</strong> The predicate condition may be a deterministic function of the trace, but it also must be invariant (i.e. remain true) under all possible executions of the body.</p><p><strong>Deterministic let expressions.</strong> We can use <code>let x = value .. end</code> to bind values to a variable, but the expression on the right-hand-side must be deterministic function of its free variables, its value must be invariant under all possible executions of the body.</p><p><strong>Stochastic let expressions.</strong> We can use <code>let x ~ dist(args...) .. end</code> to sample a stochastic value and bind to a variable, but the expression on the right-hand-side must be the application of a Gen <a href="ref/@ref"><code>Distribution</code></a> to arguments, and the distribution and its arguments must be invariant under all possible executions of the body.</p><h3><a class="nav-anchor" id="Declaring-primitive-kernels-for-use-in-composite-kernels-1" href="#Declaring-primitive-kernels-for-use-in-composite-kernels-1">Declaring primitive kernels for use in composite kernels</a></h3><p>Note that all calls to built-in kernels like <a href="#Gen.mh"><code>mh</code></a> should be stationary, but that users are also free to declare their own arbitrary code as stationary. The <a href="#Gen.@pkern"><code>@pkern</code></a> macro declares a Julia function as a stationary MCMC kernel, for use with the MCMC Kernel DSL. The following custom primitive kernel permutes the random variables using random permutation generated from outside of Gen: </p><pre><code class="language-julia">@pkern function permute_move(trace; check=false, observations=EmptyChoiceMap())
    perm = Random.randperm(trace[:n])
    constraints = choicemap()
    for (i, j) in enumerate(perm)
        constraints[(:x, i)] = trace[(:x, j)]
        constraints[(:x, j)] = trace[(:x, i)]
    end
    trace, = update(trace, (), (), constraints)
    metadata = nothing
    trace, metadata
end</code></pre><p>The first argument to the function should be the trace, and the function must have keyword arguments <code>check</code> and <code>observations</code> (see <a href="#Enabling-Dynamic-Checks-1">Enabling Dynamic Checks</a>). The return value should be a tuple where the first element is the new trace (and any remaining elements are optional metadata).</p><p><strong>Primitive kernels are Julia functions.</strong> Note that although we will be invoking these kernels within <a href="#Gen.@kern"><code>@kern</code></a> functions, these kernels can still be called like a regular Julia function.</p><pre><code class="language-julia">new_trace = permute_move(trace, 2)</code></pre><p>Indeed, they are just regular Julia functions, but with some extra information attached so that the composite kernel DSL knows they have been declared as stationary kernels.</p><h2><a class="nav-anchor" id="Reverse-Kernels-1" href="#Reverse-Kernels-1">Reverse Kernels</a></h2><p>The <strong>reversal</strong> of a stationary MCMC kernel with distribution <span>$k_1(t&#39;; t)$</span>, for model with distribution <span>$p(t; x)$</span>, is another MCMC kernel with distribution:</p><div>\[k_2(t; t&#39;) := \frac{p(t; x)}{p(t&#39;; x)} k_1(t&#39;; t)\]</div><p>For custom primitive kernels declared with <a href="#Gen.@pkern"><code>@pkern</code></a>, users can declare the reversal kernel with the <a href="#Gen.@rkern"><code>@rkern</code></a> macro:</p><pre><code class="language-julia">@rkern k1 : k2</code></pre><p>This also assigns <code>k1</code> as the reversal of <code>k2</code>. The composite kernel DSL automatically generates the reversal kernel for composite kernels, and built-in stationary kernels like <a href="#Gen.mh"><code>mh</code></a>. The reversal of a kernel (primitive or composite) can be obtained with <a href="#Gen.reversal"><code>reversal</code></a>.</p><h2><a class="nav-anchor" id="API-1" href="#API-1">API</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Gen.metropolis_hastings" href="#Gen.metropolis_hastings"><code>Gen.metropolis_hastings</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">(new_trace, accepted) = metropolis_hastings(
    trace, selection::Selection;
    check=false, observations=EmptyChoiceMap())</code></pre><p>Perform a Metropolis-Hastings update that proposes new values for the selected addresses from the internal proposal (often using ancestral sampling), returning the new trace (which is equal to the previous trace if the move was not accepted) and a Bool indicating whether the move was accepted or not.</p></div></div><a class="source-link" target="_blank" href="https://github.com/probcomp/Gen/blob/9be333c310f9478b839636797d2428daab758bf6/src/inference/mh.jl#L7-L13">source</a><div><div><pre><code class="language-none">(new_trace, accepted) = metropolis_hastings(
    trace, proposal::GenerativeFunction, proposal_args::Tuple;
    check=false, observations=EmptyChoiceMap())</code></pre><p>Perform a Metropolis-Hastings update that proposes new values for some subset of random choices in the given trace using the given proposal generative function, returning the new trace (which is equal to the previous trace if the move was not accepted) and a Bool indicating whether the move was accepted or not.</p><p>The proposal generative function should take as its first argument the current trace of the model, and remaining arguments <code>proposal_args</code>. If the proposal modifies addresses that determine the control flow in the model, values must be provided by the proposal for any addresses that are newly sampled by the model.</p></div></div><a class="source-link" target="_blank" href="https://github.com/probcomp/Gen/blob/9be333c310f9478b839636797d2428daab758bf6/src/inference/mh.jl#L31-L40">source</a><div><div><pre><code class="language-none">(new_trace, accepted) = metropolis_hastings(
    trace, proposal::GenerativeFunction, proposal_args::Tuple, involution::Function;
    check=false, observations=EmptyChoiceMap())</code></pre><p>Perform a generalized Metropolis-Hastings update based on an involution (bijection that is its own inverse) on a space of choice maps, returning the new trace (which is equal to the previous trace if the move was not accepted) and a Bool indicating whether the move was accepted or not.</p><p>The <code>involution</code> Julia function has the following signature:</p><pre><code class="language-none">(new_trace, bwd_choices::ChoiceMap, weight) = involution(trace, fwd_choices::ChoiceMap, fwd_ret, proposal_args::Tuple)</code></pre><p>The generative function <code>proposal</code> is executed on arguments <code>(trace, proposal_args...)</code>, producing a choice map <code>fwd_choices</code> and return value <code>fwd_ret</code>. For each value of model arguments (contained in <code>trace</code>) and <code>proposal_args</code>, the <code>involution</code> function applies an involution that maps the tuple <code>(get_choices(trace), fwd_choices)</code> to the tuple <code>(get_choices(new_trace), bwd_choices)</code>. Note that <code>fwd_ret</code> is a deterministic function of <code>fwd_choices</code> and <code>proposal_args</code>. When only discrete random choices are used, the <code>weight</code> must be equal to <code>get_score(new_trace) - get_score(trace)</code>.</p><p><strong>Including Continuous Random Choices</strong> When continuous random choices are used, the <code>weight</code> must include an additive term that is the determinant of the the Jacobian of the bijection on the continuous random choices that is obtained by currying the involution on the discrete random choices.</p></div></div><a class="source-link" target="_blank" href="https://github.com/probcomp/Gen/blob/9be333c310f9478b839636797d2428daab758bf6/src/inference/mh.jl#L63-L81">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Gen.mh" href="#Gen.mh"><code>Gen.mh</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">(new_trace, accepted) = mh(trace, selection::Selection; ..)
(new_trace, accepted) = mh(trace, proposal::GenerativeFunction, proposal_args::Tuple; ..)
(new_trace, accepted) = mh(trace, proposal::GenerativeFunction, proposal_args::Tuple, involution::Function; ..)</code></pre><p>Alias for <a href="#Gen.metropolis_hastings"><code>metropolis_hastings</code></a>. Perform a Metropolis-Hastings update on the given trace.</p></div></div><a class="source-link" target="_blank" href="https://github.com/probcomp/Gen/blob/9be333c310f9478b839636797d2428daab758bf6/src/inference/mh.jl#L120-L126">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Gen.mala" href="#Gen.mala"><code>Gen.mala</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">(new_trace, accepted) = mala(
    trace, selection::Selection, tau::Real;
    check=false, observations=EmptyChoiceMap())</code></pre><p>Apply a Metropolis-Adjusted Langevin Algorithm (MALA) update.</p><p><a href="https://en.wikipedia.org/wiki/Metropolis-adjusted_Langevin_algorithm">Reference URL</a></p></div></div><a class="source-link" target="_blank" href="https://github.com/probcomp/Gen/blob/9be333c310f9478b839636797d2428daab758bf6/src/inference/mala.jl#L2-L10">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Gen.hmc" href="#Gen.hmc"><code>Gen.hmc</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">(new_trace, accepted) = hmc(
    trace, selection::Selection; L=10, eps=0.1,
    check=false, observations=EmptyChoiceMap())</code></pre><p>Apply a Hamiltonian Monte Carlo (HMC) update.</p><p>Neal, Radford M. &quot;MCMC using Hamiltonian dynamics.&quot; Handbook of Markov Chain Monte Carlo 2.11 (2011): 2.</p><p><a href="http://www.mcmchandbook.net/HandbookChapter5.pdf">Reference URL</a></p></div></div><a class="source-link" target="_blank" href="https://github.com/probcomp/Gen/blob/9be333c310f9478b839636797d2428daab758bf6/src/inference/hmc.jl#L13-L23">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Gen.elliptical_slice" href="#Gen.elliptical_slice"><code>Gen.elliptical_slice</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">new_trace = elliptical_slice(
    trace, addr, mu, cov;
    check=false, observations=EmptyChoiceMap())</code></pre><p>Apply an elliptical slice sampling update to a given random choice with a multivariate normal prior.</p><p>Also takes the mean vector and covariance matrix of the prior.</p><p><a href="http://proceedings.mlr.press/v9/murray10a/murray10a.pdf">Reference URL</a></p></div></div><a class="source-link" target="_blank" href="https://github.com/probcomp/Gen/blob/9be333c310f9478b839636797d2428daab758bf6/src/inference/elliptical_slice.jl#L2-L12">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Gen.@pkern" href="#Gen.@pkern"><code>Gen.@pkern</code></a> — <span class="docstring-category">Macro</span>.</div><div><div><pre><code class="language-none">@pkern function k(trace, ..; check=false, observations=EmptyChoiceMap())
    ..
    return trace
end</code></pre><p>Declare a Julia function as a primitive stationary kernel.</p><p>The first argument of the function should be a trace, and the return value of the function should be a trace. There should be keyword arguments check and observations.</p></div></div><a class="source-link" target="_blank" href="https://github.com/probcomp/Gen/blob/9be333c310f9478b839636797d2428daab758bf6/src/inference/kernel_dsl.jl#L16-L26">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Gen.@kern" href="#Gen.@kern"><code>Gen.@kern</code></a> — <span class="docstring-category">Macro</span>.</div><div><div><pre><code class="language-none">@kern function k(trace, ..)
    ..
end</code></pre><p>Construct a composite MCMC kernel.</p><p>The resulting object is a Julia function that is annotated as a composite MCMC kernel, and can be called as a Julia function or applied within other composite kernels.</p></div></div><a class="source-link" target="_blank" href="https://github.com/probcomp/Gen/blob/9be333c310f9478b839636797d2428daab758bf6/src/inference/kernel_dsl.jl#L201-L209">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Gen.@rkern" href="#Gen.@rkern"><code>Gen.@rkern</code></a> — <span class="docstring-category">Macro</span>.</div><div><div><pre><code class="language-none">@rkern k1 : k2</code></pre><p>Declare that two primitive stationary kernels are reversals of one another.</p><p>The two kernels must have the same argument type signatures.</p></div></div><a class="source-link" target="_blank" href="https://github.com/probcomp/Gen/blob/9be333c310f9478b839636797d2428daab758bf6/src/inference/kernel_dsl.jl#L136-L142">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Gen.reversal" href="#Gen.reversal"><code>Gen.reversal</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">k2 = reversal(k1)</code></pre><p>Return the reversal kernel for a given kernel.</p></div></div><a class="source-link" target="_blank" href="https://github.com/probcomp/Gen/blob/9be333c310f9478b839636797d2428daab758bf6/src/inference/kernel_dsl.jl#L126-L130">source</a></section><footer><hr/><a class="previous" href="../importance/"><span class="direction">Previous</span><span class="title">Importance Sampling</span></a><a class="next" href="../map/"><span class="direction">Next</span><span class="title">MAP Optimization</span></a></footer></article></body></html>
