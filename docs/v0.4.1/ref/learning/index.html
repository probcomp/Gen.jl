<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Learning Generative Functions · Gen</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link href="../../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>Gen</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../../">Home</a></li><li><a class="toctext" href="../../getting_started/">Getting Started</a></li><li><a class="toctext" href="../../tutorials/">Tutorials</a></li><li><span class="toctext">Modeling Languages and APIs</span><ul><li><a class="toctext" href="../gfi/">Generative Functions</a></li><li><a class="toctext" href="../distributions/">Probability Distributions</a></li><li><a class="toctext" href="../modeling/">Built-in Modeling Language</a></li><li><a class="toctext" href="../combinators/">Generative Function Combinators</a></li><li><a class="toctext" href="../choice_maps/">Choice Maps</a></li><li><a class="toctext" href="../selections/">Selections</a></li><li><a class="toctext" href="../parameter_optimization/">Optimizing Trainable Parameters</a></li><li><a class="toctext" href="../trace_translators/">Trace Translators</a></li><li><a class="toctext" href="../extending/">Extending Gen</a></li></ul></li><li><span class="toctext">Standard Inference Library</span><ul><li><a class="toctext" href="../importance/">Importance Sampling</a></li><li><a class="toctext" href="../map/">MAP Optimization</a></li><li><a class="toctext" href="../mcmc/">Markov chain Monte Carlo</a></li><li><a class="toctext" href="../map/">MAP Optimization</a></li><li><a class="toctext" href="../pf/">Particle Filtering</a></li><li><a class="toctext" href="../vi/">Variational Inference</a></li><li class="current"><a class="toctext" href>Learning Generative Functions</a><ul class="internal"><li><a class="toctext" href="#Learning-from-Complete-Data-1">Learning from Complete Data</a></li><li><a class="toctext" href="#Learning-from-Incomplete-Data-1">Learning from Incomplete Data</a></li><li><a class="toctext" href="#References-1">References</a></li><li><a class="toctext" href="#API-1">API</a></li></ul></li></ul></li><li><span class="toctext">Internals</span><ul><li><a class="toctext" href="../internals/parameter_optimization/">Optimizing Trainable Parameters</a></li><li><a class="toctext" href="../internals/language_implementation/">Modeling Language Implementation</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li>Standard Inference Library</li><li><a href>Learning Generative Functions</a></li></ul><a class="edit-page" href="https://github.com/probcomp/Gen.jl/blob/master/docs/src/ref/learning.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Learning Generative Functions</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Learning-Generative-Functions-1" href="#Learning-Generative-Functions-1">Learning Generative Functions</a></h1><p>Learning and inference are closely related concepts, and the distinction between the two is not always clear. Often, <strong>learning</strong> refers to inferring long-lived unobserved quantities that will be reused across many problem instances (like a dynamics model for an entity that we are trying to track), whereas <strong>inference</strong> refers to inferring shorter-lived quantities (like a specific trajectory of a specific entity). Learning is the way to use data to automatically generate <strong>models</strong> of the world, or to automatically fill in unknown parameters in hand-coded models. These resulting models are then used in various inference tasks.</p><p>There are many variants of the learning task–we could be training the weights of a neural network, estimating a handful of parameters in a structured and hand-coded model, or we could be learning the structure or architecture of a model. Also, we could do Bayesian learning in which we seek a probability distribution on possible models, or we could seek just the best model, as measured by e.g. <strong>maximum likelihood</strong>. This section focuses on maximum likelihood learning of the <a href="ref/@ref">Trainable parameters</a> of a generative function. These are numerical quantities that are part of the generative function&#39;s state, with respect to which generative functions are able to report gradients of their (log) probability density function of their density function. Trainable parameters are different from random choices–random choices are per-trace and trainable parameters are a property of the generative function (which is associated with many traces). Also, unlike random choices, trainable parameters do not have a prior distribution.</p><p>There are two settings in which we might learn these parameters using maximum likelihood. If our observed data contains values for all of the random choices made by the generative function, this is called <strong>learning from complete data</strong>, and is a relatively straightforward task. If our observed data is missing values for some random choices (either because the value happened to be missing, or because it was too expensive to acquire it, or because it is an inherently unmeasurable quantity), this is called <strong>learning from incomplete data</strong>, and is a substantially harder task. Gen provides programming primitives and design patterns for both tasks. In both cases, the models we are learning can be either generative or discriminative.</p><h2><a class="nav-anchor" id="Learning-from-Complete-Data-1" href="#Learning-from-Complete-Data-1">Learning from Complete Data</a></h2><p>This section discusses maximizing the log likelihood of observed data over the space of trainable parameters, when all of the random variables are observed. In Gen, the likelihood of complete data is simply the joint probability (density) of a trace, and maximum likelihood with complete data amounts to maximizing the sum of log joint probabilities of a collection of traces <span>$t_i$</span> for <span>$i = 1\ldots N$</span> with respect to the trainable parameters of the generative function, which are denoted <span>$\theta$</span>.</p><div>\[\max_{\theta} \sum_{i=1}^N \log p(t_i; x, \theta)\]</div><p>For example, here is a simple generative model that we might want to learn:</p><pre><code class="language-julia">@gen function model()
    @param x_mu::Float64
    @param a::Float64
    @param b::Float64
    x = @trace(normal(x_mu, 1.), :x)
    @trace(normal(a * x + b, 1.), :y)
end</code></pre><p>There are three components to <span>$\theta$</span> for this generative function: <code>(x_mu, a, b)</code>.</p><p>Note that maximum likelihood can be used to learn generative and discriminative models, but for discriminative models, the arguments to the generative function will be different for each training example:</p><div>\[\max_{\theta} \sum_{i=1}^N \log p(t_i; x_i, \theta)\]</div><p>Here is a minimal discriminative model:</p><pre><code class="language-julia">@gen function disc_model(x::Float64)
    @param a::Float64
    @param b::Float64
    @trace(normal(a * x + b, 1.), :y)
end</code></pre><p>Let&#39;s suppose we are training the generative model. The first step is to initialize the values of the trainable parameters, which for generative functions constructed using the built-in modeling languages, we do with <a href="../modeling/#Gen.init_param!"><code>init_param!</code></a>:</p><pre><code class="language-julia">init_param!(model, :a, 0.)
init_param!(model, :b, 0.)</code></pre><p>Each trace in the collection contains the observed data from an independent draw from our model. We can populate each trace with its observed data using <a href="../gfi/#Gen.generate"><code>generate</code></a>:</p><pre><code class="language-julia">traces = []
for observations in data
    trace, = generate(model, model_args, observations)
    push!(traces, trace)
end</code></pre><p>For the complete data case, we assume that all random choices in the model are constrained by the observations choice map (we will analyze the case when not all random choices are constrained in the next section). We can evaluate the objective function by summing the result of <a href="../gfi/#Gen.get_score"><code>get_score</code></a> over our collection of traces:</p><pre><code class="language-julia">objective = sum([get_score(trace) for trace in traces])</code></pre><p>We can compute the gradient of this objective function with respect to the trainable parameters using <a href="../gfi/#Gen.accumulate_param_gradients!"><code>accumulate_param_gradients!</code></a>:</p><pre><code class="language-julia">for trace in traces
    accumulate_param_gradients!(trace)
end</code></pre><p>Finally, we can construct and gradient-based update with <a href="../parameter_optimization/#Gen.ParamUpdate"><code>ParamUpdate</code></a> and apply it with <a href="../parameter_optimization/#Gen.apply!"><code>apply!</code></a>. We can put this all together into a function:</p><pre><code class="language-julia">function train_model(data::Vector{ChoiceMap})
    init_param!(model, :theta, 0.1)
    traces = []
    for observations in data
        trace, = generate(model, model_args, observations)
        push!(traces, trace)
    end
    update = ParamUpdate(FixedStepSizeGradientDescent(0.001), model)
    for iter=1:max_iter
        objective = sum([get_score(trace) for trace in traces])
        println(&quot;objective: $objective&quot;)
        for trace in traces
            accumulate_param_gradients!(trace)
        end
        apply!(update)
    end
end</code></pre><p>Note that using the same primitives (<a href="../gfi/#Gen.generate"><code>generate</code></a> and <a href="../gfi/#Gen.accumulate_param_gradients!"><code>accumulate_param_gradients!</code></a>), you can compose various more sophisticated learning algorithms involving e.g. stochastic gradient descent and minibatches, and more sophisticated stochastic gradient optimizers like <a href="../parameter_optimization/#Gen.ADAM"><code>ADAM</code></a>. For example, <a href="#Gen.train!"><code>train!</code></a> trains a generative function from complete data with minibatches.</p><h2><a class="nav-anchor" id="Learning-from-Incomplete-Data-1" href="#Learning-from-Incomplete-Data-1">Learning from Incomplete Data</a></h2><p>When there are random variables in our model whose value is not observed in our data set, then doing maximum learning is significantly more difficult. Specifically, maximum likelihood is aiming to maximize the <strong>marginal likelihood</strong> of the observed data, which is an integral or sum over the values of the unobserved random variables. Let&#39;s denote the observed variables as <code>y</code> and the hidden variables as <code>z</code>:</p><div>\[\sum_{i=1}^N \log p(y_i; x, \theta) = \sum_{i=1}^N \log \left( \sum_{z_i} p(z_i, y_i; x, \theta)\right)\]</div><p>It is often intractable to evaluate this quantity for specific values of the parameters, let alone maximize it. Most techniques for learning models from incomplete data, from the EM algorithm to variational autoencoders address this problem by starting with some initial <span>$\theta = \theta_0$</span> and iterating between two steps:</p><ul><li><p>Doing inference about the hidden variables <span>$z_i$</span> given the observed variables <span>$y_i$</span>, for the model with the current values of <span>$\theta$</span>, which produces some <strong>completions</strong> of the hidden variables <span>$z_i$</span> or some representation of the posterior distribution on these hidden variables. This step does not update the parameters <span>$\theta$</span>.</p></li><li><p>Optimize the parameters <span>$\theta$</span> to maximize the data of the complete log likelihood, as in the setting of complete data. This step does not involve inference about the hidden variables <span>$z_i$</span>.</p></li></ul><p>Various algorithms can be understood as examples of this general pattern, although they differ in several details including (i) how they represent the results of inferences, (ii) how they perform the inference step, (iii) whether they try to solve each of the inference and parameter-optimization problems incrementally or not, and (iv) their formal theoretical justification and analysis:</p><ul><li><p>[Expectation maximization (EM) [1], including incremental variants [2]</p></li><li><p>Monte Carlo EM [3] and online variants [4]</p></li><li><p>Variational EM</p></li><li><p>The wake-sleep algorithm [5] and reweighted wake-sleep algorithms [6]</p></li><li><p>Variational autoencoders [7]</p></li></ul><p>In Gen, the results of inference are typically represented as a collection of traces of the model, which include values for the latent variables. The section <a href="#Learning-from-Complete-Data-1">Learning from Complete Data</a> describes how to perform the parameter update step given a collection of such traces. In the remainder of this section, we describe various learning algorithms, organized by the inference approach they take to obtain traces.</p><h3><a class="nav-anchor" id="Monte-Carlo-EM-1" href="#Monte-Carlo-EM-1">Monte Carlo EM</a></h3><p>Monte Carlo EM is a broad class of algorithms that use Monte Carlo sampling within the inference step to generate the set of traces that is used for the learning step. There are many variants possible, based on which Monte Carlo inference algorithm is used. For example:</p><pre><code class="language-julia">function train_model(data::Vector{ChoiceMap})
    init_param!(model, :theta, 0.1)
    update = ParamUpdate(FixedStepSizeGradientDescent(0.001), model)
    for iter=1:max_iter
        traces = do_monte_carlo_inference(data)
        for trace in traces
            accumulate_param_gradients!(trace)
        end
        apply!(update)
    end
end

function do_monte_carlo_inference(data)
    num_traces = 1000
    (traces, log_weights, _) = importance_sampling(model, (), data, num_samples)
    weights = exp.(log_weights)
    [traces[categorical(weights)] for _=1:num_samples]
end</code></pre><p>Note that it is also possible to use a weighted collection of traces directly without resampling:</p><pre><code class="language-julia">function train_model(data::Vector{ChoiceMap})
    init_param!(model, :theta, 0.1)
    update = ParamUpdate(FixedStepSizeGradientDescent(0.001), model)
    for iter=1:max_iter
        traces, weights = do_monte_carlo_inference_with_weights(data)
        for (trace, weight) in zip(traces, weights)
            accumulate_param_gradients!(trace, nothing, weight)
        end
        apply!(update)
    end
end</code></pre><p>MCMC and other algorithms can be used for inference as well.</p><h3><a class="nav-anchor" id="Online-Monte-Carlo-EM-1" href="#Online-Monte-Carlo-EM-1">Online Monte Carlo EM</a></h3><p>The Monte Carlo EM example performed inference from scratch within each iteration. However, if the change tothe parameters during each iteration is small, it is likely that the traces from the previous iteration can be reused. There are various ways of reusing traces: We can use the traces obtained for the previous traces to initialize MCMC for the new parameters. We can reweight the traces based on the change to their importance weights [4].</p><h3><a class="nav-anchor" id="Wake-sleep-algorithm-1" href="#Wake-sleep-algorithm-1">Wake-sleep algorithm</a></h3><p>The wake-sleep algorithm [5] is an approach to training generative models that uses an <em>inference network</em>, a neural network that takes in the values of observed random variables and returns parameters of a probability distribution on latent variables. We call the conditional probability distribution on the latent variables, given the observed variables, the <em>inference model</em>. In Gen, both the generative model and the inference model are represented as generative functions. The wake-sleep algorithm trains the inference model as it trains the generative model. At each iteration, during the <em>wake phase</em>, the generative model is trained on complete traces generated by running the current version of the inference on the observed data. At each iteration, during the <em>sleep phase</em>, the inference model is trained on data generated by simulating from the current generative model. The <a href="#Gen.lecture!"><code>lecture!</code></a> or <a href="#Gen.lecture_batched!"><code>lecture_batched!</code></a> methods can be used for the sleep phase training.</p><h3><a class="nav-anchor" id="Reweighted-wake-sleep-algorithm-1" href="#Reweighted-wake-sleep-algorithm-1">Reweighted wake-sleep algorithm</a></h3><p>The reweighted wake-sleep algorithm [6] is an extension of the wake-sleep algorithm, where during the wake phase, for each observation, a collection of latent completions are taken by simulating from the inference model multiple times. Then, each of these is weighted by an importance weight. This extension can be implemented with <a href="../importance/#Gen.importance_sampling"><code>importance_sampling</code></a>.</p><h3><a class="nav-anchor" id="Variational-inference-1" href="#Variational-inference-1">Variational inference</a></h3><p>Variational inference can be used to for the inference step. Here, the parameters of the variational approximation, represented as a generative function, are fit to the posterior during the inference step. <a href="../vi/#Gen.black_box_vi!"><code>black_box_vi!</code></a> or <a href="../vi/#Gen.black_box_vimco!"><code>black_box_vimco!</code></a> can be used to fit the variational approximation. Then, the traces of the model can be obtained by simulating from the variational approximation and merging the resulting choice maps with the observed data.</p><h3><a class="nav-anchor" id="Amortized-variational-inference-(VAEs)-1" href="#Amortized-variational-inference-(VAEs)-1">Amortized variational inference (VAEs)</a></h3><p>Instead of fitting the variational approximation from scratch for each observation, it is possible to fit an <em>inference model</em> instead, that takes as input the observation, and generates a distribution on latent variables as output (as in the wake sleep algorithm). When we train the variational approximation by minimizing the evidence lower bound (ELBO) this is called amortized variational inference. Variational autencoders are an example. It is possible to perform amortized variational inference using <a href="ref/@ref"><code>black_box_vi</code></a> or <a href="../vi/#Gen.black_box_vimco!"><code>black_box_vimco!</code></a>.</p><h2><a class="nav-anchor" id="References-1" href="#References-1">References</a></h2><p>[1] Dempster, Arthur P., Nan M. Laird, and Donald B. Rubin. &quot;Maximum likelihood from incomplete data via the EM algorithm.&quot; Journal of the Royal Statistical Society: Series B (Methodological) 39.1 (1977): 1-22. <a href="https://users.fmrib.ox.ac.uk/~jesper/papers/readgroup_070213/DLR_on_EM.pdf">Link</a></p><p>[2] Neal, Radford M., and Geoffrey E. Hinton. &quot;A view of the EM algorithm that justifies incremental, sparse, and other variants.&quot; Learning in graphical models. Springer, Dordrecht, 1998. 355-368. <a href="https://www.cs.toronto.edu/~radford/ftp/emk.pdf">Link</a></p><p>[3] Wei, Greg CG, and Martin A. Tanner. &quot;A Monte Carlo implementation of the EM algorithm and the poor man&#39;s data augmentation algorithms.&quot; Journal of the American statistical Association 85.411 (1990): 699-704. <a href="http://www.biostat.jhsph.edu/~rpeng/biostat778/papers/wei-tanner-1990.pdf">Link</a></p><p>[4] Levine, Richard A., and George Casella. &quot;Implementations of the Monte Carlo EM algorithm.&quot; Journal of Computational and Graphical Statistics 10.3 (2001): 422-439. <a href="https://amstat.tandfonline.com/doi/abs/10.1198/106186001317115045">Link</a></p><p>[5] Hinton, Geoffrey E., et al. &quot;The&quot; wake-sleep&quot; algorithm for unsupervised neural networks.&quot; Science 268.5214 (1995): 1158-1161. <a href="https://science.sciencemag.org/content/sci/268/5214/1158.full.pdf">Link</a></p><p>[6] Jorg Bornschein and Yoshua Bengio. Reweighted wake sleep. ICLR 2015. <a href="https://arxiv.org/pdf/1406.2751.pdf">Link</a></p><p>[7] Diederik P. Kingma, Max Welling: Auto-Encoding Variational Bayes. ICLR 2014 <a href="https://arxiv.org/pdf/1312.6114.pdf">Link</a></p><h2><a class="nav-anchor" id="API-1" href="#API-1">API</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Gen.lecture!" href="#Gen.lecture!"><code>Gen.lecture!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">score = lecture!(
    p::GenerativeFunction, p_args::Tuple,
    q::GenerativeFunction, get_q_args::Function)</code></pre><p>Simulate a trace of p representing a training example, and use to update the gradients of the trainable parameters of q.</p><p>Used for training q via maximum expected conditional likelihood. Random choices will be mapped from p to q based on their address. get<em>q</em>args maps a trace of p to an argument tuple of q. score is the conditional log likelihood (or an unbiased estimate of a lower bound on it, if not all of q&#39;s random choices are constrained, or if q uses non-addressable randomness).</p></div></div><a class="source-link" target="_blank" href="https://github.com/probcomp/Gen.jl/blob/a6fb962bb169700762228cb66d542575a61c3aea/src/inference/train.jl#L85-L96">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Gen.lecture_batched!" href="#Gen.lecture_batched!"><code>Gen.lecture_batched!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">score = lecture_batched!(
    p::GenerativeFunction, p_args::Tuple,
    q::GenerativeFunction, get_q_args::Function)</code></pre><p>Simulate a batch of traces of p representing training samples, and use them to update the gradients of the trainable parameters of q.</p><p>Like <code>lecture!</code> but q is batched, and must make random choices for training sample i under hierarchical address namespace i::Int (e.g. i =&gt; :z). get<em>q</em>args maps a vector of traces of p to an argument tuple of q.</p></div></div><a class="source-link" target="_blank" href="https://github.com/probcomp/Gen.jl/blob/a6fb962bb169700762228cb66d542575a61c3aea/src/inference/train.jl#L107-L116">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Gen.train!" href="#Gen.train!"><code>Gen.train!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">train!(gen_fn::GenerativeFunction, data_generator::Function,
       update::ParamUpdate,
       num_epoch, epoch_size, num_minibatch, minibatch_size; verbose::Bool=false)</code></pre><p>Train the given generative function to maximize the expected conditional log probability (density) that <code>gen_fn</code> generates the assignment <code>constraints</code> given inputs, where the expectation is taken under the output distribution of <code>data_generator</code>.</p><p>The function <code>data_generator</code> is a function of no arguments that returns a tuple <code>(inputs, constraints)</code> where <code>inputs</code> is a <code>Tuple</code> of inputs (arguments) to <code>gen_fn</code>, and <code>constraints</code> is an <code>ChoiceMap</code>.</p><p><code>conf</code> configures the optimization algorithm used.</p><p><code>param_lists</code> is a map from generative function to lists of its parameters. This is equivalent to minimizing the expected KL divergence from the conditional distribution <code>constraints | inputs</code> of the data generator to the distribution represented by the generative function, where the expectation is taken under the marginal distribution on <code>inputs</code> determined by the data generator.</p></div></div><a class="source-link" target="_blank" href="https://github.com/probcomp/Gen.jl/blob/a6fb962bb169700762228cb66d542575a61c3aea/src/inference/train.jl#L1-L23">source</a></section><footer><hr/><a class="previous" href="../vi/"><span class="direction">Previous</span><span class="title">Variational Inference</span></a><a class="next" href="../internals/parameter_optimization/"><span class="direction">Next</span><span class="title">Optimizing Trainable Parameters</span></a></footer></article></body></html>
