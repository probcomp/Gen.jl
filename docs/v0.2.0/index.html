<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Gen Introduction · Gen</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>Gen</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li class="current"><a class="toctext" href>Gen Introduction</a><ul class="internal"><li><a class="toctext" href="#Generative-Functions,-Traces,-and-Assignments-1">Generative Functions, Traces, and Assignments</a></li><li><a class="toctext" href="#Implementing-Inference-Algorithms-1">Implementing Inference Algorithms</a></li><li><a class="toctext" href="#Implementing-Gradient-Based-Learning-1">Implementing Gradient-Based Learning</a></li><li><a class="toctext" href="#Probabilistic-Modules-1">Probabilistic Modules</a></li><li><a class="toctext" href="#Compiled-Generative-Functions-1">Compiled Generative Functions</a></li><li><a class="toctext" href="#Incremental-Computation-1">Incremental Computation</a></li><li><a class="toctext" href="#Higher-Order-Probabilistic-Modules-1">Higher-Order Probabilistic Modules</a></li><li><a class="toctext" href="#Using-metaprogramming-to-implement-new-inference-algorithms-1">Using metaprogramming to implement new inference algorithms</a></li></ul></li><li><a class="toctext" href="documentation/">Gen Documentation</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Gen Introduction</a></li></ul><a class="edit-page" href="https://github.com/probcomp/Gen/blob/master/docs/src/index.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Gen Introduction</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Gen-Introduction-1" href="#Gen-Introduction-1">Gen Introduction</a></h1><p>Gen is an extensible and reasonably performant probabilistic computing platform that makes it easier to develop probabilistic inference and learning applications.</p><h2><a class="nav-anchor" id="Generative-Functions,-Traces,-and-Assignments-1" href="#Generative-Functions,-Traces,-and-Assignments-1">Generative Functions, Traces, and Assignments</a></h2><p>Stochastic generative processes are represented in Gen as <em>generative functions</em>. Generative functions are Julia functions that have been annotated using the <code>@gen</code> macro. The generative function below takes a vector of x-coordinates, randomly generates the slope and intercept parameters of a line, and returns a random vector of y-coordinates sampled near that line at the given x-coordinates:</p><pre><code class="language-julia">@gen function regression_model(xs::Vector{Float64})
    slope = normal(0, 2)
    intercept = normal(0, 2)
    ys = Vector{Float64}(undef, length(xs))
    for (i, x) in enumerate(xs)
        ys[i] = normal(slope * xs + intercept, 1.)
    end
    return ys
end</code></pre><p>We can evaluate the generative function:</p><pre><code class="language-julia">ys = regression_model([-5.0, -3.0, 0.0, 3.0, 5.0])</code></pre><p>Above we have used a generative function to implement a simulation. However, what distinguishes generative functions from plain-old simulators is their ability to be <em>traced</em>. When we <em>trace</em> a generative function, we record the random choices that it makes, as well as additional data about the evaluation of the function. This capability makes it possible to implement algorithms for probabilistic inference. To trace a random choice, we need to give it a unique <em>address</em>, using the <code>@addr</code> keyword. Here we give addresses to each of the random choices:</p><pre><code class="language-julia">@gen function regression_model(xs::Vector{Float64})
    slope = @addr(normal(0, 2), &quot;slope&quot;)
    intercept = @addr(normal(0, 2), &quot;intercept&quot;)
    ys = Vector{Float64}(undef, length(xs))
    for (i, x) in enumerate(xs)
        ys[i] = @addr(normal(slope * xs[i] + intercept, 1.), &quot;y-$i&quot;)
    end
    return ys
end</code></pre><p>Addresses can be arbitrary Julia values except for <code>Pair</code>. Julia symbols, strings, and integers are common types to use for addresses.</p><p>We trace a generative function using the <code>simulate</code> method. We provide the arguments to the function in a tuple:</p><pre><code class="language-julia">xs = [-5.0, -3.0, 0.0, 3.0]
trace = simulate(regression_model, (xs,))</code></pre><p>The trace that is returned is a form of stack trace of the generative function that contains, among other things, the values for the random choices that were annotated with <code>@addr</code>. To extract the values of the random choices from a trace, we use the method <code>get_assmt</code>:</p><pre><code class="language-julia">assignment = get_assmt(trace)</code></pre><p>The <code>get_assmt</code> method returns an <em>assignment</em>, which is a trie (prefix tree) that contains the values of random choices. Printing the assignment gives a pretty-printed representation:</p><pre><code class="language-julia">print(assignment)</code></pre><pre><code class="language-none">│
├── &quot;y-2&quot; : -1.7800101128038626
│
├── &quot;y-3&quot; : 0.1832573462320619
│
├── &quot;intercept&quot; : 1.7434641799887896
│
├── &quot;y-4&quot; : 5.074512278024528
│
├── &quot;slope&quot; : 1.5232349541190595
│
└── &quot;y-1&quot; : -4.978881121779669</code></pre><p>Generative functions can also call other generative functions; these calls can also be traced using <code>@addr</code>:</p><pre><code class="language-julia">@gen function generate_params()
    slope = @addr(normal(0, 2), &quot;slope&quot;)
    intercept = @addr(normal(0, 2), &quot;intercept&quot;)
    return (slope, intercept)
end

@gen function generate_datum(x, slope, intercept)
    return @addr(normal(slope * x + intercept, 1.), &quot;y&quot;)
end

@gen function regression_model(xs::Vector{Float64})
    (slope, intercept) = @addr(generate_params(), &quot;parameters&quot;)
    ys = Vector{Float64}(undef, length(xs))
    for (i, x) in enumerate(xs)
        ys[i] = @addr(generate_datum(xs[i], slope, intercept), i)
    end
    return ys
end</code></pre><p>This results in a hierarchical assignment:</p><pre><code class="language-julia">trace = simulate(regression_model, (xs,))
assignment = get_assmt(trace)
print(assignment)</code></pre><pre><code class="language-none">│
├── 2
│   │
│   └── &quot;y&quot; : -3.264252749715529
│
├── 3
│   │
│   └── &quot;y&quot; : -2.3036480286819865
│
├── &quot;parameters&quot;
│   │
│   ├── &quot;intercept&quot; : -0.8767368668034233
│   │
│   └── &quot;slope&quot; : 0.9082675922758383
│
├── 4
│   │
│   └── &quot;y&quot; : 2.4971551239517695
│
└── 1
    │
    └── &quot;y&quot; : -7.561723378403817</code></pre><p>We can read values from a assignment using the following syntax:</p><pre><code class="language-julia">assignment[&quot;intercept&quot;]</code></pre><p>To read the value at a hierarchical address, we provide a <code>Pair</code> where the first element of the pair is the first part ofthe hierarchical address, and the second element is the rest of the address. For example:</p><pre><code class="language-julia">assignment[1 =&gt; &quot;y&quot;]</code></pre><p>Julia provides the operator <code>=&gt;</code> for constructing <code>Pair</code> values. Long hierarchical addresses can be constructed by chaining this operator, which associates right:</p><pre><code class="language-julia">assignment[1 =&gt; &quot;y&quot; =&gt; :foo =&gt; :bar]</code></pre><p>Generative functions can also write to hierarchical addresses directly:</p><pre><code class="language-julia">@gen function regression_model(xs::Vector{Float64})
    slope = @addr(normal(0, 2), &quot;slope&quot;)
    intercept = @addr(normal(0, 2), &quot;intercept&quot;)
    ys = Vector{Float64}(undef, length(xs))
    for (i, x) in enumerate(xs)
        ys[i] = @addr(normal(slope * xs[i] + intercept, 1.), i =&gt; &quot;y&quot;)
    end
    return ys
end</code></pre><pre><code class="language-julia">trace = simulate(regression_model, (xs,))
assignment = get_assmt(trace)
print(assignment)</code></pre><pre><code class="language-none">│
├── &quot;intercept&quot; : -1.340778590777462
│
├── &quot;slope&quot; : -2.0846094796654686
│
├── 2
│   │
│   └── &quot;y&quot; : 3.64234023192473
│
├── 3
│   │
│   └── &quot;y&quot; : -1.5439406188116667
│
├── 4
│   │
│   └── &quot;y&quot; : -8.655741483764384
│
└── 1
    │
    └── &quot;y&quot; : 9.451320138931484</code></pre><h2><a class="nav-anchor" id="Implementing-Inference-Algorithms-1" href="#Implementing-Inference-Algorithms-1">Implementing Inference Algorithms</a></h2><h2><a class="nav-anchor" id="Implementing-Gradient-Based-Learning-1" href="#Implementing-Gradient-Based-Learning-1">Implementing Gradient-Based Learning</a></h2><h2><a class="nav-anchor" id="Probabilistic-Modules-1" href="#Probabilistic-Modules-1">Probabilistic Modules</a></h2><h2><a class="nav-anchor" id="Compiled-Generative-Functions-1" href="#Compiled-Generative-Functions-1">Compiled Generative Functions</a></h2><h2><a class="nav-anchor" id="Incremental-Computation-1" href="#Incremental-Computation-1">Incremental Computation</a></h2><p>Getting good asymptotic scaling for iterative local search algorithms like MCMC or MAP optimization relies on the ability to update a trace efficiently in two common scenarios: (i) when a small number of random choice(s) are changed; or (ii) when there is a small change to the arguments of the function.</p><p>To enable efficient trace updates, generative functions use <em>argdiffs</em> and <em>retdiffs</em>:</p><ul><li>An <em>argdiff</em> describes the change made to the arguments of the generative function, relative to the arguments in the previous trace.</li><li>A <em>retdiff</em> describes the change to the return value of a generative function, relative to the return value in the previous trace.</li></ul><p>The update generative function API methods <code>update</code>, <code>fix_update</code>, and <code>extend</code> accept the argdiff value, alongside the new arguments to the function, the previous trace, and other parameters; and return the new trace and the retdiff value.</p><h3><a class="nav-anchor" id="Argdiffs-1" href="#Argdiffs-1">Argdiffs</a></h3><p>An argument difference value, or <em>argdiff</em>, is associated with a pair of argument tuples <code>args::Tuple</code> and <code>new_args::Tuple</code>. The update methods for a generative function accept different types of argdiff values, that depend on the generative function. Two singleton data types are provided:</p><ul><li><code>noargdiff::NoArgDiff</code>, for expressing that there is no difference to the argument.</li><li><code>unknownargdiff::UnknownArgDiff</code>, for expressing that there is an unknown difference in the arguments.</li></ul><p>Generative functions may or may not accept these types as argdiffs, depending on the generative function.</p><h3><a class="nav-anchor" id="Retdiffs-1" href="#Retdiffs-1">Retdiffs</a></h3><p>A return value difference value, or <em>retdiff</em>, is associated with a pair of traces. The update methods for a generative function return different types of retdiff values, depending on the generative function. The only requirement placed on retdiff values is that they implement the <code>isnodiff</code> method, which takes a retdiff value and returns <code>true</code> if there was no change in the return value, and otherwise returns false.</p><h3><a class="nav-anchor" id="Custom-incremental-computation-in-embedded-modeling-DSL-1" href="#Custom-incremental-computation-in-embedded-modeling-DSL-1">Custom incremental computation in embedded modeling DSL</a></h3><p>&lt;!– TODO: This section is a bit confusing, in particular the macros @choicediff and @calldiff appear suddenly and are not clear or well-motivated. Should introduce them earlier and explain more.–&gt;</p><p>We now show how argdiffs and retdiffs and can be used for incremental computation in the embedded modeling DSL. For generative functions expressed in the embedded modeling DSL, retdiff values are computed by user <em>diff code</em> that is placed inline in the body of the generative function definition. Diff code consists of Julia statements that can depend on non-diff code, but non-diff code cannot depend on the diff code. To distinguish <em>diff code</em>  from regular code in the generative function, the <code>@diff</code> macro is placed in front of the statement, e.g.:</p><pre><code class="language-julia">x = y + 1
@diff foo = 2
y = x</code></pre><p>Diff code is only executed during update methods such as <code>update</code>, <code>fix_update</code>, or <code>extend</code> methods. In other methods that are not associated with an update to a trace (e.g. <code>generate</code>, <code>simulate</code>, <code>assess</code>), the diff code is removed from the body of the generative function. Therefore, the body of the generative function with the diff code removed must still be a valid generative function.</p><p>Unlike non-diff code, diff code has access to the argdiff value (using <code>@argdiff()</code>), and may invoke <code>@retdiff(&lt;value&gt;)</code>, which sets the retdiff value. Diff code also has access to information about the change to the values of random choices and the change to the return values of calls to other generative functions. Changes to the return values of random choices are queried using <code>@choicediff(&lt;addr&gt;)</code>, which must be invoked after the <code>@addr</code> expression for that address, and returns one of the following values:</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Gen.NewChoiceDiff" href="#Gen.NewChoiceDiff"><code>Gen.NewChoiceDiff</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">NewChoiceDiff()</code></pre><p>Singleton indicating that there was previously no random choice at this address.</p></div></div><a class="source-link" target="_blank" href="https://github.com/probcomp/Gen/blob/543b03cac5e261ba28dcf71ec6ec2722ee1d2e87/src/diff.jl#L112-L116">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Gen.NoChoiceDiff" href="#Gen.NoChoiceDiff"><code>Gen.NoChoiceDiff</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">NoChoiceDiff()</code></pre><p>Singleton indicating that the value of the random choice did not change.</p></div></div><a class="source-link" target="_blank" href="https://github.com/probcomp/Gen/blob/543b03cac5e261ba28dcf71ec6ec2722ee1d2e87/src/diff.jl#L122-L126">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Gen.PrevChoiceDiff" href="#Gen.PrevChoiceDiff"><code>Gen.PrevChoiceDiff</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">PrevChoiceDiff(prev)</code></pre><p>Wrapper around the previous value of the random choice indicating that it may have changed.</p></div></div><a class="source-link" target="_blank" href="https://github.com/probcomp/Gen/blob/543b03cac5e261ba28dcf71ec6ec2722ee1d2e87/src/diff.jl#L132-L136">source</a></section><p>Diff code also has access to the retdiff values associated with calls it makes to generative functions, using <code>@calldiff(&lt;addr&gt;)</code>, which returns a value of one of the following types:</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Gen.NewCallDiff" href="#Gen.NewCallDiff"><code>Gen.NewCallDiff</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">NewCallDiff()</code></pre><p>Singleton indicating that there was previously no call at this address.</p></div></div><a class="source-link" target="_blank" href="https://github.com/probcomp/Gen/blob/543b03cac5e261ba28dcf71ec6ec2722ee1d2e87/src/diff.jl#L62-L66">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Gen.NoCallDiff" href="#Gen.NoCallDiff"><code>Gen.NoCallDiff</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">NoCallDiff()</code></pre><p>Singleton indicating that the return value of the call has not changed.</p></div></div><a class="source-link" target="_blank" href="https://github.com/probcomp/Gen/blob/543b03cac5e261ba28dcf71ec6ec2722ee1d2e87/src/diff.jl#L73-L77">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Gen.UnknownCallDiff" href="#Gen.UnknownCallDiff"><code>Gen.UnknownCallDiff</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">UnknownCallDiff()</code></pre><p>Singleton indicating that there was a previous call at this address, but that no information is known about the change to the return value.</p></div></div><a class="source-link" target="_blank" href="https://github.com/probcomp/Gen/blob/543b03cac5e261ba28dcf71ec6ec2722ee1d2e87/src/diff.jl#L84-L88">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Gen.CustomCallDiff" href="#Gen.CustomCallDiff"><code>Gen.CustomCallDiff</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">CustomCallDiff(retdiff)</code></pre><p>Wrapper around a retdiff value, indicating that there was a previous call at this address, and that <code>isnodiff(retdiff) = false</code> (otherwise <code>NoCallDiff()</code> would have been returned).</p></div></div><a class="source-link" target="_blank" href="https://github.com/probcomp/Gen/blob/543b03cac5e261ba28dcf71ec6ec2722ee1d2e87/src/diff.jl#L95-L99">source</a></section><p>Diff code can also pass argdiff values to generative functions that it calls, using the third argument in an <code>@addr</code> expression, which is always interpreted as diff code (depsite the absence of a <code>@diff</code> keyword).</p><pre><code class="language-julia">@diff my_argdiff = @argdiff()
@diff argdiff_for_foo = ..
@addr(foo(arg1, arg2), addr, argdiff_for_foo)
@diff retdiff_from_foo = @calldiff(addr)
@diff @retdiff(..)</code></pre><h2><a class="nav-anchor" id="Higher-Order-Probabilistic-Modules-1" href="#Higher-Order-Probabilistic-Modules-1">Higher-Order Probabilistic Modules</a></h2><h2><a class="nav-anchor" id="Using-metaprogramming-to-implement-new-inference-algorithms-1" href="#Using-metaprogramming-to-implement-new-inference-algorithms-1">Using metaprogramming to implement new inference algorithms</a></h2><p>Many Monte Carlo inference algorithms, like Hamiltonian Monte Carlo (HMC) and Metropolis-Adjusted Langevin Algorithm (MALA) are instances of general inference algorithm templates like Metropolis-Hastings, with specialized proposal distributions. These algorithms can therefore be implemented with high-performance for a model if a compiled generative function defining the proposal is constructed manually. However, it is also possible to write a generic implementation that automatically generates the generative function for the proposal using Julia&#39;s metaprogramming capabilities. This section shows a simple example of writing a procedure that generates the code needed for a MALA update applied to an arbitrary set of top-level static addresses.</p><p>First, we write out a non-generic implementation of MALA. MALA uses a proposal that tends to propose values in the direction of the gradient. The procedure will be hardcoded to act on a specific set of addresses for a model called <code>model</code>.</p><pre><code class="language-julia">set = DynamicAddressSet()
for addr in [:slope, :intercept, :inlier_std, :outlier_std]
    Gen.push_leaf_node!(set, addr)
end
mala_selection = StaticAddressSet(set)

@compiled @gen function mala_proposal(prev, tau)
    std::Float64 = sqrt(2*tau)
    gradients::StaticChoiceTrie = backprop_trace(model, prev, mala_selection, nothing)[3]
    @addr(normal(get_assmt(prev)[:slope] + tau * gradients[:slope], std), :slope)
    @addr(normal(get_assmt(prev)[:intercept] + tau * gradients[:intercept], std), :intercept)
    @addr(normal(get_assmt(prev)[:inlier_std] + tau * gradients[:inlier_std], std), :inlier_std)
    @addr(normal(get_assmt(prev)[:outlier_std] + tau * gradients[:outlier_std], std), :outlier_std)
end

mala_move(trace, tau::Float64) = mh(model, mala_proposal, (tau,), trace)</code></pre><p>Next, we write a generic version that takes a set of addresses and generates the implementation for that set. This version only works on a set of static top-level addresses.</p><pre><code class="language-julia">function generate_mala_move(model, addrs)

    # create selection
    set = DynamicAddressSet()
    for addr in addrs
        Gen.push_leaf_node!(set, addr)
    end
    selection = StaticAddressSet(set)

    # generate proposal function
    stmts = Expr[]
    for addr in addrs
        quote_addr = QuoteNode(addr)
        push!(stmts, :(
            @addr(normal(get_assmt(prev)[$quote_addr] + tau * gradients[$quote_addr], std),
                  $quote_addr)
        ))
    end
    mala_proposal_name = gensym(&quot;mala_proposal&quot;)
    mala_proposal = eval(quote
        @compiled @gen function $mala_proposal_name(prev, tau)
            gradients::StaticChoiceTrie = backprop_trace(
                model, prev, $(QuoteNode(selection)), nothing)[3]
            std::Float64 = sqrt(2*tau)
            $(stmts...)
        end
    end)

    return (trace, tau::Float64) -&gt; mh(model, mala_proposal, (tau,), trace)
end
</code></pre><p>We can then use the generate the MALA move for a speciic model and specific addresses using:</p><pre><code class="language-julia">mala_move = generate_mala_move(model, [:slope, :intercept, :inlier_std, :outlier_std])</code></pre><footer><hr/><a class="next" href="documentation/"><span class="direction">Next</span><span class="title">Gen Documentation</span></a></footer></article></body></html>
