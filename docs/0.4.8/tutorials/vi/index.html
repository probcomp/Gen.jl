<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Variational Inference in Gen · Gen.jl</title><meta name="title" content="Variational Inference in Gen · Gen.jl"/><meta property="og:title" content="Variational Inference in Gen · Gen.jl"/><meta property="twitter:title" content="Variational Inference in Gen · Gen.jl"/><meta name="description" content="Documentation for Gen.jl."/><meta property="og:description" content="Documentation for Gen.jl."/><meta property="twitter:description" content="Documentation for Gen.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Gen.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Gen.jl</a></li><li><input class="collapse-toggle" id="menuitem-2" type="checkbox" checked/><label class="tocitem" for="menuitem-2"><span class="docs-label">Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../getting_started/">Getting Started</a></li><li><a class="tocitem" href="../modeling_in_gen/">Introduction to Modeling in Gen</a></li><li><a class="tocitem" href="../mcmc_map/">Basics of MCMC and MAP Inference</a></li><li><a class="tocitem" href="../enumerative/">Debugging Models with Enumeration</a></li><li><a class="tocitem" href="../smc/">Object Tracking with SMC</a></li><li class="is-active"><a class="tocitem" href>Variational Inference in Gen</a><ul class="internal"><li><a class="tocitem" href="#A-Simple-Example-of-VI"><span>A Simple Example of VI</span></a></li><li><a class="tocitem" href="#Posterior-Inference-with-VI"><span>Posterior Inference with VI</span></a></li><li><a class="tocitem" href="#Amortized-Variational-Inference"><span>Amortized Variational Inference</span></a></li><li><a class="tocitem" href="#Reparametrization-Trick"><span>Reparametrization Trick</span></a></li></ul></li><li><a class="tocitem" href="../learning_gen_fns/">Learning Generative Functions</a></li><li><a class="tocitem" href="../scaling_with_sml/">Speeding Up Inference with the SML</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">How-to Guides</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../how_to/extending_gen/">Extending Gen</a></li><li><a class="tocitem" href="../../how_to/custom_distributions/">Adding New Distributions</a></li><li><a class="tocitem" href="../../how_to/custom_gen_fns/">Adding New Generative Functions</a></li><li><a class="tocitem" href="../../how_to/custom_gradients/">Custom Gradients</a></li><li><a class="tocitem" href="../../how_to/custom_incremental_computation/">Custom Incremental Computation</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">Reference</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><input class="collapse-toggle" id="menuitem-4-1" type="checkbox"/><label class="tocitem" for="menuitem-4-1"><span class="docs-label">Core Interfaces</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../ref/core/gfi/">Generative Function Interface</a></li><li><a class="tocitem" href="../../ref/core/choice_maps/">Choice Maps</a></li><li><a class="tocitem" href="../../ref/core/selections/">Selections</a></li><li><a class="tocitem" href="../../ref/core/change_hints/">Change Hints</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-2" type="checkbox"/><label class="tocitem" for="menuitem-4-2"><span class="docs-label">Modeling Library</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../ref/modeling/dml/">Built-In Modeling Language</a></li><li><a class="tocitem" href="../../ref/modeling/sml/">Static Modeling Language</a></li><li><a class="tocitem" href="../../ref/modeling/distributions/">Probability Distributions</a></li><li><a class="tocitem" href="../../ref/modeling/combinators/">Combinators</a></li><li><a class="tocitem" href="../../ref/modeling/custom_gen_fns/">Custom Generative Functions</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-3" type="checkbox"/><label class="tocitem" for="menuitem-4-3"><span class="docs-label">Inference Library</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../ref/inference/enumerative/">Enumerative Inference</a></li><li><a class="tocitem" href="../../ref/inference/importance/">Importance Sampling</a></li><li><a class="tocitem" href="../../ref/inference/mcmc/">Markov Chain Monte Carlo</a></li><li><a class="tocitem" href="../../ref/inference/pf/">Particle Filtering &amp; SMC</a></li><li><a class="tocitem" href="../../ref/inference/trace_translators/">Trace Translators</a></li><li><a class="tocitem" href="../../ref/inference/parameter_optimization/">Parameter Optimization</a></li><li><a class="tocitem" href="../../ref/inference/map/">MAP Optimization</a></li><li><a class="tocitem" href="../../ref/inference/vi/">Variational Inference</a></li><li><a class="tocitem" href="../../ref/inference/wake_sleep/">Wake-Sleep Learning</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-4" type="checkbox"/><label class="tocitem" for="menuitem-4-4"><span class="docs-label">Internals</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../ref/internals/language_implementation/">Modeling Language Implementation</a></li></ul></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Variational Inference in Gen</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Variational Inference in Gen</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/probcomp/Gen.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/probcomp/Gen.jl/blob/master/docs/src/tutorials/vi.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="vi_tutorial"><a class="docs-heading-anchor" href="#vi_tutorial">Variational Inference in Gen</a><a id="vi_tutorial-1"></a><a class="docs-heading-anchor-permalink" href="#vi_tutorial" title="Permalink"></a></h1><p>Variational inference (VI) involves optimizing the parameters of a variational family to maximize a lower bound on the marginal likelihood called the ELBO. In Gen, variational families are represented as generative functions, and variational inference typically involves optimizing the trainable parameters of generative functions.</p><h2 id="A-Simple-Example-of-VI"><a class="docs-heading-anchor" href="#A-Simple-Example-of-VI">A Simple Example of VI</a><a id="A-Simple-Example-of-VI-1"></a><a class="docs-heading-anchor-permalink" href="#A-Simple-Example-of-VI" title="Permalink"></a></h2><p>Let&#39;s begin with a simple example that illustrates how to use Gen&#39;s <a href="../../ref/inference/vi/#Gen.black_box_vi!"><code>black_box_vi!</code></a> function to perform variational inference. In variational inference, we have a target distribution <span>$P(x)$</span> that we wish to approximate with some variational distribution <span>$Q(x; \phi)$</span> with trainable parameters <span>$\phi$</span>.</p><p>In many cases, this target distribution is a posterior distribution <span>$P(x | y)$</span> given a fixed set of observations <span>$y$</span>. But in this example, we assume we know <span>$P(x)$</span> exactly, and optimize <span>$\phi$</span> so that <span>$Q(x; \phi)$</span> fits <span>$P(x)$</span>.</p><p>We first define the <strong>target distribution</strong> <span>$P(x)$</span> as a normal distribution with  with a mean of <code>-1</code> and a standard deviation of <code>exp(0.5)</code>:</p><pre><code class="language-julia hljs">@gen function target()
    x ~ normal(-1, exp(0.5))
end</code></pre><p>We now define a <strong>variational family</strong>, also known as a <em>guide</em>, as a generative function <span>$Q(x; \phi)$</span> parameterized by a set of trainable parameters <span>$\phi$</span>. This requires (i) picking the functional form of the variational distribution (e.g. normal, Cauchy, etc.), (ii) choosing how the distribution is parameterized.</p><p>Our target distribution is normal, so we make our variational family normally distributed as well. We also define two variational parameters, <code>x_mu</code> and <code>x_log_std</code>, which are the mean and log standard deviation of our variational distribution.</p><pre><code class="language-julia hljs">@gen function approx()
    @param x_mu::Float64
    @param x_log_std::Float64
    x ~ normal(x_mu, exp(x_log_std))
end</code></pre><p>Since <code>x_mu</code> and <code>x_log_std</code> are not fixed to particular values, this generative function defines a <em>family</em> of distributions, not just one. Note that we intentionally chose to parameterize the distribution by the log standard deviation <code>x_log_std</code>, so that every parameter has full support over the real line, and we can perform unconstrained optimization of the parameters.</p><p>To perform variational inference, we need to initialize the variational parameters to their starting values:</p><pre><code class="language-julia hljs">init_param!(approx, :x_mu, 0.0)
init_param!(approx, :x_log_std, 0.0)</code></pre><p>Now we can use the <a href="../../ref/inference/vi/#Gen.black_box_vi!"><code>black_box_vi!</code></a> function to perform variational inference using <a href="../../ref/inference/parameter_optimization/#Gen.GradientDescent"><code>GradientDescent</code></a> to update the variational parameters.</p><pre><code class="language-julia hljs">observations = choicemap()
param_update = ParamUpdate(GradientDescent(1., 1000), approx)
black_box_vi!(target, (), observations, approx, (), param_update;
              iters=200, samples_per_iter=100, verbose=false)</code></pre><p>We can now inspect the resulting variational parameters, and see if we have recovered the parameters of the target distribution:</p><pre><code class="language-julia hljs">x_mu = get_param(approx, :x_mu)
x_log_std = get_param(approx, :x_log_std)
@show x_mu x_log_std;</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">x_mu = -1.0
x_log_std = 0.5</code></pre><p>As expected, we have recovered the parameters of the target distribution.</p><h2 id="Posterior-Inference-with-VI"><a class="docs-heading-anchor" href="#Posterior-Inference-with-VI">Posterior Inference with VI</a><a id="Posterior-Inference-with-VI-1"></a><a class="docs-heading-anchor-permalink" href="#Posterior-Inference-with-VI" title="Permalink"></a></h2><p>In the above example, we used a target distribution <span>$P(x)$</span> that we had full knowledge about. When performing posterior inference, however, we typically only have the ability to sample from a generative model <span>$x, y \sim P(x) P(y | x)$</span>, and to evaluate the joint probability <span>$P(x, y)$</span>, but not the ability to evaluate or sample from the posterior <span>$P(x | y)$</span> for a fixed obesrvation <span>$y$</span>.</p><p>Variational inference can address this by approximating <span>$P(x | y)$</span> with <span>$Q(x; \phi)$</span>, allowing us to sample and evaluate <span>$Q(x; \phi)$</span> instead. This is done by maximizing a quantity known as the <strong>evidence lower bound</strong> or <strong>ELBO</strong>, which is a lower bound on the log marginal likelihood <span>$\log P(y)$</span> of the observations <span>$y$</span>. The ELBO can be written in multiple equivalent forms:</p><p class="math-container">\[\begin{aligned}
\operatorname{ELBO}(\phi; y)
&amp;= \mathbb{E}_{x \sim Q(x; \phi)}\left[\log \frac{P(x, y)}{Q(x; \phi)}\right] \\
&amp;= \mathbb{E}_{x \sim Q(x; \phi)}[\log P(x, y)] + \operatorname{H}[Q(x; \phi)] \\
&amp;= \log P(y) - \operatorname{KL}[Q(x; \phi) || P(x | y)]
\end{aligned}\]</p><p>Here, <span>$\operatorname{H}[Q(x; \phi)]$</span> is the entropy of the variational distribution <span>$Q(x; \phi)$</span>, and <span>$\operatorname{KL}[Q(x; \phi) || P(x | y)]$</span> is the Kullback-Leibler divergence between the variational distribution <span>$Q(x; \phi)$</span> and the target distribution <span>$P(x | y)$</span>. From the third line, we can see that the ELBO is a lower bound on <span>$\log P(y)$</span>, and that maximizing the ELBO is equivalent to minimizing the KL divergence between <span>$Q(x; \phi)$</span> and <span>$P(x | y)$</span>.</p><p>Let&#39;s test this for a generative model <span>$P(x, y)$</span> where it is possible (with a bit of work) to analytically calculate the posterior <span>$P(y | x)$</span>:</p><pre><code class="language-julia hljs">@gen function model(n::Int)
    x ~ normal(0, 1)
    for i in 1:n
        {(:y, i)} ~ normal(x, 0.5)
    end
end</code></pre><p>In this normal-normal model, an unknown mean <span>$x$</span> is sampled from a <span>$\operatorname{Normal}(0, 1)$</span> prior. Then we draw <span>$n$</span> datapoints <span>$y_{1:n}$</span> from a normal distribution centered around <span>$x$</span> with a standard deviation of 0.5. Our task is to infer the posterior distribution over <span>$x$</span> given that we have observed <span>$y_{1:n}$</span>. We&#39;ll reuse the same variational family as before:</p><pre><code class="language-julia hljs">@gen function approx()
    @param x_mu::Float64
    @param x_log_std::Float64
    x ~ normal(x_mu, exp(x_log_std))
end</code></pre><p>Suppose we observe <span>$n = 6$</span> datapoints <span>$y_{1:6}$</span> with the following values:</p><pre><code class="language-julia hljs">ys = [3.12, 2.25, 2.21, 1.55, 2.15, 1.06]</code></pre><p>It is possible to show analytically that the posterior <span>$P(x | y_{1:n})$</span> is normally distributed with mean <span>$\mu_n = \frac{4n}{1 + 4n} \bar y$</span> and standard deviation <span>$\sigma_n = \frac{1}{\sqrt{1 + 4n}}$</span>, where <span>$\bar y$</span> is the mean of <span>$y_{1:n}$</span>:</p><pre><code class="language-julia hljs">n = length(ys)
x_mu_expected = 4*n / (1 + 4*n) * (sum(ys) / n)
x_std_expected = 1/(sqrt((1 + 4*n)))
@show x_mu_expected x_std_expected;</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">x_mu_expected = 1.9744000000000004
x_std_expected = 0.2</code></pre><p>Let&#39;s see whether variational inference can reproduce these values. We first construct a choicemap of our observations:</p><pre><code class="language-julia hljs">observations = choicemap()
for (i, y) in enumerate(ys)
    observations[(:y, i)] = y
end</code></pre><p>Next, we configure our <a href="../../ref/inference/parameter_optimization/#Gen.GradientDescent"><code>GradientDescent</code></a> optimizer. Since this is a more complicated optimization proplem, we use a smaller initial step size of 0.01:</p><pre><code class="language-julia hljs">step_size_init = 0.01
step_size_beta = 1000
update_config = GradientDescent(step_size_init, step_size_beta)</code></pre><p>We then initialize the parameters of our variational approximation, and pass our model, observations, and variational family to <a href="../../ref/inference/vi/#Gen.black_box_vi!"><code>black_box_vi!</code></a>. </p><pre><code class="language-julia hljs">init_param!(approx, :x_mu, 0.0)
init_param!(approx, :x_log_std, 0.0)
param_update = ParamUpdate(update_config, approx);
elbo_est, _, elbo_history =
    black_box_vi!(model, (n,), observations, approx, (), param_update;
                  iters=500, samples_per_iter=200, verbose=false);</code></pre><p>As expected, the ELBO estimate increases over time, eventually converging to a value around -9.9:</p><pre><code class="language-julia hljs">for t in [1; 50:50:500]
    println(&quot;iter $(lpad(t, 3)): elbo est. = $(elbo_history[t])&quot;)
end
println(&quot;final elbo est. = $elbo_est&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">iter   1: elbo est. = -70.82821502297315
iter  50: elbo est. = -9.917444455675646
iter 100: elbo est. = -9.913643238760457
iter 150: elbo est. = -9.950928571713582
iter 200: elbo est. = -9.904603359402396
iter 250: elbo est. = -9.908764220027615
iter 300: elbo est. = -9.943135475124667
iter 350: elbo est. = -9.897202989529985
iter 400: elbo est. = -9.923144272738394
iter 450: elbo est. = -9.924233691481495
iter 500: elbo est. = -9.895938393401352
final elbo est. = -9.895938393401352</code></pre><p>Inspecting the resulting variational parameters, we find that they are reasonable approximations to the parameters of the true posterior:</p><pre><code class="language-julia hljs">x_mu_approx = get_param(approx, :x_mu)
Δx_mu = x_mu_approx - x_mu_expected

x_log_std_approx = get_param(approx, :x_log_std)
x_std_approx = exp(x_log_std_approx)
Δx_std = x_std_approx - x_std_expected

@show (x_mu_approx, Δx_mu) (x_std_approx, Δx_std);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(x_mu_approx, Δx_mu) = (1.9095081602106097, -0.06489183978939073)
(x_std_approx, Δx_std) = (0.18248775256393504, -0.01751224743606497)</code></pre><h2 id="Amortized-Variational-Inference"><a class="docs-heading-anchor" href="#Amortized-Variational-Inference">Amortized Variational Inference</a><a id="Amortized-Variational-Inference-1"></a><a class="docs-heading-anchor-permalink" href="#Amortized-Variational-Inference" title="Permalink"></a></h2><p>In standard variational inference, we have to optimize the variational parameters <span>$\phi$</span> for each new inference problem. Depending on how difficult the optimization problem is, this may be costly.</p><p>As an alternative, we can perform <strong>amortized variational inference</strong>: Instead of optimizing <span>$\phi$</span> for each set of observations <span>$y$</span> that we encounter, we learn a <em>function</em> <span>$f_\varphi(y)$</span> that outputs a set of distribution parameters <span>$\phi_y$</span> for each <span>$y$</span>, and optimize the parameters of the function <span>$\varphi$</span>. We do this over a dataset of <span>$K$</span> independently distributed observation sets <span>$Y = \{y^1, ..., y^K\}$</span>, maximizing the expected ELBO over this dataset:</p><p class="math-container">\[\begin{aligned}
\operatorname{A-ELBO}(\varphi; Y)
&amp;= \frac{1}{K} \sum_{k=1}^{K} \operatorname{ELBO}(\varphi; y^k) \\
&amp;= \frac{1}{K} \left[\log P(Y) - \sum_{k=1}^{K} \operatorname{KL}[Q(x; f_{\varphi}(y^k)) || P(x | y^k)] \right]
\end{aligned}\]</p><p>We will perform amortized VI over the same generative <code>model</code> we defined earlier:</p><pre><code class="language-julia hljs">@gen function model(n::Int)
    x ~ normal(0, 1)
    for i in 1:n
        {(:y, i)} ~ normal(x, 0.5)
    end
end</code></pre><p>Since amortized VI is performed over a dataset of <code>K</code> observation sets <span>$\{y^1, ..., y^K\}$</span>, where each <span>$y^k$</span> has <span>$n$</span> datapoints <span>$(y^k_1, ..., y^k_n)$</span> , we need to nest <code>model</code> within a <a href="../../ref/modeling/combinators/#Gen.Map"><code>Map</code></a> combinator that repeats <code>model</code> <span>$K$</span> times:</p><pre><code class="language-julia hljs">mapped_model = Map(model)</code></pre><p>Let&#39;s generate a synthetic dataset of <span>$K = 10$</span> observation sets, each with <span>$n = 6$</span> datapoints:</p><pre><code class="language-julia hljs"># Simulate 10 observation sets of length 6
K, n = 10, 6
mapped_trace = simulate(mapped_model, (fill(n, K),))
observations = get_choices(mapped_trace)

# Select just the `y` values, excluding the generated `x` values
sel = select((k =&gt; (:y, i) for i in 1:n for k in 1:K)...)
observations = get_selected(observations, sel)
all_ys = [[observations[k =&gt; (:y, i)] for i in 1:n] for k in 1:K]</code></pre><p>Now let&#39;s define our amortized approximation, which takes in an observation set <code>ys</code>, and computes the parameters of a normal distribution over <code>x</code> as a function of <code>ys</code>:</p><pre><code class="language-julia hljs">@gen function amortized_approx(ys)
    @param x_mu_bias::Float64
    @param x_mu_coeff::Float64
    @param x_log_std::Float64
    x_mu = x_mu_bias + x_mu_coeff * sum(ys)
    x ~ normal(x_mu, exp(x_log_std))
    return (x_mu, x_log_std)
end</code></pre><p>Similar to our <code>model</code>, we need to wrap this variational approximation in a <a href="../../ref/modeling/combinators/#Gen.Map"><code>Map</code></a> combinator:</p><pre><code class="language-julia hljs">mapped_approx = Map(amortized_approx)</code></pre><p>In our choice of function <span>$f_\varphi(y)$</span>, we exploit the fact that the posterior mean <code>x_mu</code> should depend on the sum of the values in <code>ys</code>, along with the knowledge that <code>x_log_std</code> does not depend on <code>ys</code>. We could have chosen a more complex function, such as full-rank linear regression, or a neural network, but this would make optimization more difficult. Given this choice of function, the optimal parameters <span>$\varphi^*$</span> can be computed analytically:</p><pre><code class="language-julia hljs">n = 6

x_mu_bias_optimal = 0.0
x_mu_coeff_optimal = 4 / (1 + 4*n)

x_std_optimal = 1/(sqrt((1 + 4*n)))
x_log_std_optimal = log(x_std_optimal)

@show x_mu_bias_optimal x_mu_coeff_optimal x_log_std_optimal;</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">x_mu_bias_optimal = 0.0
x_mu_coeff_optimal = 0.16
x_log_std_optimal = -1.6094379124341003</code></pre><p>We can now fit our variational approximation via <a href="../../ref/inference/vi/#Gen.black_box_vi!"><code>black_box_vi!</code></a>: We initialize the variational parameters, then configure our parameter update to update the parameters of <code>amortized_approx</code>:</p><pre><code class="language-julia hljs"># Configure parameter update to optimize the parameters of `amortized_approx`
step_size_init = 1e-4
step_size_beta = 1000
update_config = GradientDescent(step_size_init, step_size_beta)

# Initialize the amortized variational parameters, then the parameter update
init_param!(amortized_approx, :x_mu_bias, 0.0);
init_param!(amortized_approx, :x_mu_coeff, 0.0);
init_param!(amortized_approx, :x_log_std, 0.0);
param_update = ParamUpdate(update_config, amortized_approx);

# Run amortized black-box variational inference over the synthetic observations
mapped_model_args = (fill(n, K), )
mapped_approx_args = (all_ys, )
elbo_est, _, elbo_history =
    black_box_vi!(mapped_model, mapped_model_args, observations,
                  mapped_approx, mapped_approx_args, param_update;
                  iters=500, samples_per_iter=100, verbose=false);</code></pre><p>Once again, the ELBO estimate increases and eventually converges:</p><pre><code class="language-julia hljs">for t in [1; 50:50:500]
    println(&quot;iter $(lpad(t, 3)): elbo est. = $(elbo_history[t])&quot;)
end
println(&quot;final elbo est. = $elbo_est&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">iter   1: elbo est. = -255.96030322843262
iter  50: elbo est. = -78.03264195140899
iter 100: elbo est. = -66.2775440617424
iter 150: elbo est. = -65.12844115255216
iter 200: elbo est. = -64.90124666222313
iter 250: elbo est. = -71.15398672328172
iter 300: elbo est. = -72.01383675412625
iter 350: elbo est. = -57.972927270283556
iter 400: elbo est. = -58.34364124479296
iter 450: elbo est. = -56.905884899333294
iter 500: elbo est. = -56.45664188772582
final elbo est. = -56.45664188772582</code></pre><p>Our amortized variational parameters <span>$\varphi$</span> are also fairly close to their optimal values <span>$\varphi^*$</span>:</p><pre><code class="language-julia hljs">x_mu_bias = get_param(amortized_approx, :x_mu_bias)
Δx_mu_bias = x_mu_bias - x_mu_bias_optimal

x_mu_coeff = get_param(amortized_approx, :x_mu_coeff)
Δx_mu_coeff = x_mu_coeff - x_mu_coeff_optimal

x_log_std = get_param(amortized_approx, :x_log_std)
Δx_log_std = x_log_std - x_log_std_optimal

@show (x_mu_bias, Δx_mu_bias) (x_mu_coeff, Δx_mu_coeff) (x_log_std, Δx_log_std);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(x_mu_bias, Δx_mu_bias) = (0.013542864726969143, 0.013542864726969143)
(x_mu_coeff, Δx_mu_coeff) = (0.16351286049325411, 0.0035128604932541108)
(x_log_std, Δx_log_std) = (-1.3532336918558392, 0.2562042205782611)</code></pre><p>If we now call <code>amortized_approx</code> with our observation set <code>ys</code> from the previous section, we should get something close to what standard variational inference produced by optimizing the paramaters of <code>approx</code> directly: </p><pre><code class="language-julia hljs">x_mu_amortized, x_log_std_amortized = amortized_approx(ys)
x_std_amortized = exp(x_log_std_amortized)

@show x_mu_amortized x_std_amortized;
@show x_mu_approx x_std_approx;
@show x_mu_expected x_std_expected;</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">x_mu_amortized = 2.0312915632137254
x_std_amortized = 0.2584033114735113
x_mu_approx = 1.9095081602106097
x_std_approx = 0.18248775256393504
x_mu_expected = 1.9744000000000004
x_std_expected = 0.2</code></pre><p>Both amortized VI and standard VI produce parameter estimates that are reasonably close to the paramters of the true posterior.</p><h2 id="Reparametrization-Trick"><a class="docs-heading-anchor" href="#Reparametrization-Trick">Reparametrization Trick</a><a id="Reparametrization-Trick-1"></a><a class="docs-heading-anchor-permalink" href="#Reparametrization-Trick" title="Permalink"></a></h2><p>To use the reparametrization trick to reduce the variance of gradient estimators, users currently need to write two versions of their variational family, one that is reparametrized and one that is not. Gen.jl does not currently include inference library support for this. We plan to add automated support for reparametrization and other variance reduction techniques in the future.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../smc/">« Object Tracking with SMC</a><a class="docs-footer-nextpage" href="../learning_gen_fns/">Learning Generative Functions »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.14.1 on <span class="colophon-date" title="Saturday 12 July 2025 00:53">Saturday 12 July 2025</span>. Using Julia version 1.10.10.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
