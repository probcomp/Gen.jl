<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Optimizing Trainable Parameters · Gen</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link href="../../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>Gen</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../../">Home</a></li><li><a class="toctext" href="../../getting_started/">Getting Started</a></li><li><a class="toctext" href="../../tutorials/">Tutorials</a></li><li><span class="toctext">Language and API Reference</span><ul><li><a class="toctext" href="../gfi/">Generative Functions</a></li><li><a class="toctext" href="../distributions/">Probability Distributions</a></li><li><a class="toctext" href="../modeling/">Built-in Modeling Language</a></li><li><a class="toctext" href="../combinators/">Generative Function Combinators</a></li><li><a class="toctext" href="../choice_maps/">Choice Maps</a></li><li><a class="toctext" href="../selections/">Selections</a></li><li class="current"><a class="toctext" href>Optimizing Trainable Parameters</a><ul class="internal"><li><a class="toctext" href="#Parameter-update-1">Parameter update</a></li><li><a class="toctext" href="#Update-configurations-1">Update configurations</a></li></ul></li><li><a class="toctext" href="../inference/">Inference Library</a></li></ul></li><li><span class="toctext">Internals</span><ul><li><a class="toctext" href="../internals/parameter_optimization/">Optimizing Trainable Parameters</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li>Language and API Reference</li><li><a href>Optimizing Trainable Parameters</a></li></ul><a class="edit-page" href="https://github.com/probcomp/Gen/blob/master/docs/src/ref/parameter_optimization.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Optimizing Trainable Parameters</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Optimizing-Trainable-Parameters-1" href="#Optimizing-Trainable-Parameters-1">Optimizing Trainable Parameters</a></h1><p>Trainable parameters of generative functions are initialized differently depending on the type of generative function. Trainable parameters of the built-in modeling language are initialized with <a href="../modeling/#Gen.init_param!"><code>init_param!</code></a>.</p><p>Gradient-based optimization of the trainable parameters of generative functions is based on interleaving two steps:</p><ul><li><p>Incrementing gradient accumulators for trainable parameters by calling <a href="../gfi/#Gen.accumulate_param_gradients!"><code>accumulate_param_gradients!</code></a> on one or more traces.</p></li><li><p>Updating the value of trainable parameters and resetting the gradient accumulators to zero, by calling <a href="#Gen.apply!"><code>apply!</code></a> on a <em>parameter update</em>, as described below.</p></li></ul><h2><a class="nav-anchor" id="Parameter-update-1" href="#Parameter-update-1">Parameter update</a></h2><p>A <em>parameter update</em> reads from the gradient accumulators for certain trainable parameters, updates the values of those parameters, and resets the gradient accumulators to zero. A paramter update is constructed by combining an <em>update configuration</em> with the set of trainable parameters to which the update should be applied:</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Gen.ParamUpdate" href="#Gen.ParamUpdate"><code>Gen.ParamUpdate</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">update = ParamUpdate(conf, param_lists...)</code></pre><p>Return an update configured by <code>conf</code> that applies to set of parameters defined by <code>param_lists</code>.</p><p>Each element in <code>param_lists</code> value is is pair of a generative function and a vector of its parameter references.</p><p><strong>Example</strong>. To construct an update that applies a gradient descent update to the parameters <code>:a</code> and <code>:b</code> of generative function <code>foo</code> and the parameter <code>:theta</code> of generative function <code>:bar</code>:</p><pre><code class="language-julia">update = ParamUpdate(GradientDescent(0.001, 100), foo =&gt; [:a, :b], bar =&gt; [:theta])</code></pre><hr/><p>Syntactic sugar for the constructor form above.</p><pre><code class="language-none">update = ParamUpdate(conf, gen_fn::GenerativeFunction)</code></pre><p>Return an update configured by <code>conf</code> that applies to all trainable parameters owned by the given generative function.</p><p>Note that trainable parameters not owned by the given generative function will not be updated, even if they are used during execution of the function.</p><p><strong>Example</strong>. If generative function <code>foo</code> has parameters <code>:a</code> and <code>:b</code>, to construct an update that applies a gradient descent update to the parameters <code>:a</code> and <code>:b</code>:</p><pre><code class="language-julia">update = ParamUpdate(GradientDescent(0.001, 100), foo)</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/probcomp/Gen/blob/f297e0b981f8d9969a676767514c1de761b3d40d/src/optimization.jl#L18-L45">source</a></section><p>The set of possible update configurations is described in <a href="#Update-configurations-1">Update configurations</a>. An update is applied with:</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Gen.apply!" href="#Gen.apply!"><code>Gen.apply!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">apply!(update::ParamUpdate)</code></pre><p>Perform one step of the update.</p></div></div><a class="source-link" target="_blank" href="https://github.com/probcomp/Gen/blob/f297e0b981f8d9969a676767514c1de761b3d40d/src/optimization.jl#L63-L67">source</a></section><h2><a class="nav-anchor" id="Update-configurations-1" href="#Update-configurations-1">Update configurations</a></h2><p>Gen has built-in support for the following types of update configurations.</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Gen.FixedStepGradientDescent" href="#Gen.FixedStepGradientDescent"><code>Gen.FixedStepGradientDescent</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">conf = FixedStepGradientDescent(step_size)</code></pre><p>Configuration for stochastic gradient descent update with fixed step size.</p></div></div><a class="source-link" target="_blank" href="https://github.com/probcomp/Gen/blob/f297e0b981f8d9969a676767514c1de761b3d40d/src/optimization.jl#L75-L79">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Gen.GradientDescent" href="#Gen.GradientDescent"><code>Gen.GradientDescent</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">conf = GradientDescent(step_size_init, step_size_beta)</code></pre><p>Configuration for stochastic gradient descent update with step size given by <code>(t::Int) -&gt; step_size_init * (step_size_beta + 1) / (step_size_beta + t)</code> where <code>t</code> is the iteration number.</p></div></div><a class="source-link" target="_blank" href="https://github.com/probcomp/Gen/blob/f297e0b981f8d9969a676767514c1de761b3d40d/src/optimization.jl#L84-L88">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Gen.ADAM" href="#Gen.ADAM"><code>Gen.ADAM</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">conf = ADAM(learning_rate, beta1, beta2, epsilon)</code></pre><p>Configuration for ADAM update.</p></div></div><a class="source-link" target="_blank" href="https://github.com/probcomp/Gen/blob/f297e0b981f8d9969a676767514c1de761b3d40d/src/optimization.jl#L94-L98">source</a></section><p>For adding new types of update configurations, see <a href="../internals/parameter_optimization/#optimizing-internal-1">Optimizing Trainable Parameters (Internal)</a>.</p><footer><hr/><a class="previous" href="../selections/"><span class="direction">Previous</span><span class="title">Selections</span></a><a class="next" href="../inference/"><span class="direction">Next</span><span class="title">Inference Library</span></a></footer></article></body></html>
