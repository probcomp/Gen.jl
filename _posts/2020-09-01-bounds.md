---
layout: post 
title: Estimating upper bounds on inference approximation error
author: Marco Cusumano-Towner
summary: "A technique for estimating upper bounds on KL divergences to the posterior."
github_comments_issueid: 1
assets: "/assets/posts/2020-09-01-bounds"
---

A key feature of probabilistic inference as a paradigm for AI is that programs
that implement probabilistic inference have well-defined mathematical
specifications for their desired behavior.
When generative models are used as the basis of inference, the generative model together with the observed data defines a specification for the desired behavior of an inference program---the inference program should encode the conditional distribution on the latent variables given the values of the observed variables.
But, probabilistic inference algorithms are often approximate, and it is not straightforward to evaluate _how well_ an inference program actually meets its specification.
This is a problem for all algorithmic approaches to approximate inference, including variational and Monte Carlo algorithms, discriminative models, and hybrids of these.

This is the first in a series of blog posts that will explore techniques for estimating how accurately a given inference program approximates a conditional distribution, focusing on techniques that I have personally used in my research.
I will use code examples in the [Gen probabilistic programming system](https://www.gen.dev) to illustrate the techniques.
This post focuses on evaluating the approximation error of variational inference programs on simulated data sets by estimating upper bounds on the (inclusive) Kullback-Leibler divergence to the conditional distribution.

## An example generative model

The techniques I'll describe are relevant to inference in any generative model, but I'll use a simplified robotics application as motivation.
Suppose we are a robot moving around in a 2D plane, and we are trying to search for and retrieve a certain object.
Suppose we have an object detector that returns the heading angle in the plane of the object detection, relative to the heading of the robot, and suppose the robot has some prior beliefs about the location of the object.
In order to rationally move through the environment in search of the object, we need to keep track of our (unceratin) beliefs about the location of the object over time as we accumulate more observations.

Let's start with a much simplified generative model of this scenario that includes a single time-step and no other objects in the environment, implemented using Gen.
The code below defines a generative model of the object's location `(x, y)` and the noisy heading measurement `obs` returned by the object detector.
The robot is assumed to lie at the origin `(0, 0)` and to be facing down the positive x-axis.

{% highlight julia %}
{% raw %}
@gen function heading_model()
    x ~ normal(1.0, 1.0)
    y ~ normal(0.0, 1.0)
    theta = atan(y, x)
    obs ~ von_mises(theta, 50.0)
end
{% endraw %}
{% endhighlight %}

Let's walk through this code briefly.
The first two random choices, `x` and `y`, are the coordinates of the object.
Here, the robot has a prior belief that the object is probably located somewhere in front of the robot.
The plot below shows some samples of of `(x, y)` from the prior distribution:

![prior-samples-of-object-location]({{ page.assets }}/no-occlusion-prior-overlay.png){:class="img-responsive" width="300px"}
{: style="text-align: center;"}

Next, `theta = atan(y, x)` computes the angle in radians of the object relative to the origin.
Finally, we sample a random choice (`obs`) that represents the observed heading from a [von Mises distribution](https://en.wikipedia.org/wiki/Von_Mises_distribution) centered at the true angle.
The plots below show several samples the prior, where the object location is a red dot, the true heading angle is shown as a blue line, and the observed heading angle is a dotted line:

![prior-samples-of-object-location-and-heading]({{ page.assets }}/no-occlusion-prior.png){:class="img-responsive" width="800px"}
{: style="text-align: center;"}

Now, given an observed angle `obs`, we want to write an inference program that approximates the conditional distribution on `x` and `y`.

## What does the (approximate) posterior look like?

Because this is a simple low-dimensional model, we can actually get a good approximation to the posterior distribution using sampling importance resampling (SIR) and the prior as the proposal.
This simple inference program is trivial to implement in Gen:
{% highlight julia %}
{% raw %}
samples = []
for i in 1:100
    trace, _ = Gen.importance_resampling(heading_model, (), Gen.choicemap((:obs, pi/4)), 10000)
    push!(samples, (trace[:x], trace[:y]))
end 
{% endraw %}
{% endhighlight %}
(Note that we could have also used `Gen.importance_sampling` and resampled a collection of 100 particles in proportion to the weights instead; that would have been more efficient and would give similar results in this case.)

Below are approximate samples of the object's location, given `obs` is $\pi/4$, and 10000 particles.

![example-posterior-samples]({{ page.assets }}/single-posterior-1.png){:class="img-responsive" width="300px"}
{: style="text-align: center;"}

This result seems to make sense qualitatively.

## Applying variational inference 

Let's now try to apply variational inference to this problem.
We will use Gen's support for [black box variational inference](https://www.gen.dev/docs/dev/ref/vi/#Black-box-variational-inference-1), which is a class of algorithms introduced by Rajesh Ranganath et al. in a [2013 paper](https://arxiv.org/abs/1401.0118) that requires only the ability to evaluate the unnormalized log probability density of the model.
Gen lets you apply black box variational inference using variational approximating families that are themselves defined as probabilistic programs.

The first step is to write the probabilistic program that defines the variational approximating family that we will optimize to match the posterior as closely as possible.
To keep things simple, I'll use a simple approximating family consisting of axis-aligned multivariate normal distributions:

{% highlight julia %}
{% raw %}
@gen function q_axis_aligned_gaussian()
    @param x_mu::Float64
    @param y_mu::Float64
    @param x_log_std::Float64
    @param y_log_std::Float64
    x ~ normal(x_mu, exp(x_log_std))
    y ~ normal(y_mu, exp(y_log_std))
end
{% endraw %}
{% endhighlight %}

Then, we can apply Gen's black box variational inference procedure to optimize the four variational parameters (`x_mu`, `y_mu`, `x_log_std`, and `y_log_std`), which are all initialized to zero:

{% highlight julia %}
{% raw %}
Gen.init_param!(q, :x_mu, 0.0)
Gen.init_param!(q, :y_mu, 0.0)
Gen.init_param!(q, :x_log_std, 0.0)
Gen.init_param!(q, :y_log_std, 0.0)
update = Gen.ParamUpdate(Gen.FixedStepGradientDescent(0.001), q_axis_aligned_gaussian)
(elbo_estimate, _, _) = Gen.black_box_vi!(
    heading_model, (50.0,), Gen.choicemap((:obs, obs)),
    q_axis_aligned_gaussian, (), update; iters=1000, samples_per_iter=100, verbose=false)
{% endraw %}
{% endhighlight %}

After optimizing the parameters, we can sample from the approximating distribution and compare the approximate posterior samples from importance sampling (left) with the approximate posterior samples from the variational approximation (right):

![example-posterior-samples]({{ page.assets }}/single-posterior-1.png){:class="img-responsive" width="300px"}
![example-variational-samples]({{ page.assets }}/single-posterior-bbvi-1.png){:class="img-responsive" width="300px"}
{: style="text-align: center;"}

There is a clear qualitative difference in the inferences.
Let's compare the approximate posteriors obtained from importance sampling and variational inference when `obs` is $\pi/2$:

![example-posterior-samples-2]({{ page.assets }}/single-posterior-2.png){:class="img-responsive" width="300px"}
![example-variational-samples-2]({{ page.assets }}/single-posterior-bbvi-2.png){:class="img-responsive" width="300px"}
{: style="text-align: center;"}

It looks like maybe the variational approximation is more accurate
for the inference problem when `obs` is $\pi/2$ than when `obs` is $\pi/4$.
This does make sense; our variational approximating family
assumes the two coordinate dimensions are independent, and this assumption appears
less valid when `obs` is $\pi/4$.
I'll now discuss a quantitative technique that can reveal this type of disparity in variational approximation accuracy.

## Quantifying the error of a variational approximation

In order to understand when our variational approximations are more or less accurate,
and in order to make rational decisions about choices of variational approximating family and how
much programming effort and computational resources to expend improving inference,
we need to be able to evaluate or estimate
the approximation error of a given variational approximation.

For the simple example above, we can qualitatively discern differences in the approximating distributions of the two algorithms, but the comparison has (at least) two issues:
It is not quantitative, and comparing against the importance sampling results is fraught since we can't be sure that the importance sampling algorithm is actually giving us accurate results either.
These problems are magnified in more realistic higher-dimensional problems.

In the remainder of this post, I'll introduce a technique that addresses both of these issues, and
then demonstrate the technique on this simple inference problem.
Prepare to see some math!

#### Evidence lower bound and exclusive KL divergence

Black box variational inference is based a clear objective function, and we will use this objective function 
as the starting point for a quantitative evaluation of the approximation error.
Let's denote latent variables by $z$, observed variables by $x$, variational parameters by $\theta$, the density function for the variational approximating family by $q(\cdot; \theta)$ and the generative model joint density by $p(z, x)$.

Briefly, black box variational inference attempts to maximize the 'evidence lower bound' or 'ELBO':

$$
\displaystyle
\max_{\theta} \mathbb{E}_{z \sim q(\cdot; \theta)}\left[ \log \frac{p(z, x)}{q(z; \theta)} \right]
= \max_{\theta} \left[ \log p(x) - \mathrm{KL}(q(\cdot; \theta) || p(\cdot | z)) \right]
$$

This is equivalent to minimizing the exclusive KL divergence:

$$
\min_{\theta} \mathrm{KL}(q(\cdot; \theta) || p(\cdot | z))
$$

This KL divergence is a reasonable choice of approximation error metric to use to evaluate variational approximations.
However, while is straightforward to estimate the ELBO by sampling from the variational approximation, because
the the actual log marginal likelihood $\log p(x)$ is unknown, it is not straightforward to estimate this KL divergence itself.

To illustrate the issue, I fit the variational family defined above for a grid of observation values, and plot the resulting ELBO estimates below:

![elbo-estimates-only]({{ page.assets }}/elbos-only.png){:class="img-responsive" width="500px"}
{: style="text-align: center;"}

While this plot looks interesting, it does't really give us much information about how accurate our variational approximation is.
For any given observed angle, we don't know the gap between the ELBO and the actual log marginal likelihood, so we can't tell from this plot whether it is worth improving our variational approximation or not.
Also, the plot doesn't tell us how the accuracy of the variational approximation depends on the observation.
While it looks like the ELBO is higher for observations near the x-axis, we don't know how much to attribute this to higher marginal likelihood for these observations versus lower KL divergence.

We could use what we believe is a more accurate estimator of the log marginal likelihood like annealed importance sampling (or, for this problem, importance sampling), and then take differences between those estimates and the ELBO estimates.
But that approach relies on trusting that the more accurate estimator is sufficiently accurate, which is undesirable.

### Stochastic lower and upper bounds on the log marginal likelihood

Next, we'll look into some math and derive a technique that gives us estimates of upper bounds on the log marginal likelihood, and show how to implement the technique using Gen.
The technique builds on an idea proposed in the 2015 paper
[Sandwiching the marginal likelihood using bidirectional Monte Carlo](https://arxiv.org/abs/1511.02543) by 
Roger Grosse, Zoubin Ghahramani, and Ryan Adams.

The technique is based on two facts regarding marginal likelihood estimators:

- Any nonnegative unbiased estimator of the marginal likelihood gives a stochastic lower bound on the log marginal likelihood. That is, if $\widehat{p}(x)$ is a non-negative random variable and $\mathbb{E}[\widehat{p}(x)] = p(x)$, then 

$$
\mathrm{Pr}(\log \widehat{p}(x) < \log p(x) + \delta) \ge 1 - \exp(-\delta)
$$

- Any nonnegative unbiased estimator of the _reciprocal_ of the marginal likelihood gives a stochastic _upper_ bound on the log marginal likelihood.
That is, if $\widehat{p}(x)$ is a non-negative random variable and $\mathbb{E}[1 / \widehat{p}(x)] = 1/p(x)$, then

$$
\mathrm{Pr}(\log \widehat{p}(x) > \log p(x) - \delta) \ge 1 - \exp(-\delta)
$$

Both of these facts can be proven using straightforward application of
[Markov's inequality](https://en.wikipedia.org/wiki/Markov%27s_inequality).

For example, for some observations $x$, if we obtain estimates $\hat{p}_1(x)$ and $\hat{p}_2(x)$ from an unbiased and unbiased reciprocal estimators respectively, then:

$$
\mathrm{Pr}(p(x) \in [\log \hat{p}_1(x) - 4, \log \hat{p}_2(x) + 4]) \ge 1 - 2 \exp(-4) \approx 0.96
$$

(Note that this is a frequentist probability and not a Bayesian statement of subjective belief about $p(x)$.)

Many of the most popular marginal likelihood estimators, including importance sampling, annealed importance sampling, and sequential Monte Carlo, are unbiased; and can therefore be used to obtain stochastic lower bounds.
For example, importance sampling can be used to construct an unbiased estimate of the marginal likelihood, using $N$ independent samples from a proposal distribution $q(\cdot)$:

$$
\widehat{p}(x) := \frac{1}{N} \sum_{i=1}^N \frac{p(z_i, x)}{q(z_i)} \;\; \mbox{for} \;\; z_i \sim q(\cdot)
$$

There are a number of asymptotically consistent unbiased estimators of the reciprocal of the marginal likelihood that can be constructed given access to a single posterior sample.
Grosse et al. 2015 describe one based on running annealed importance sampling (AIS) in reverse, starting from an exact posterior sample,
that is straightforward to implement using Gen (and, along with regular AIS, is part of Gen's core inference library).

I'll now introduce another estimator that gives stochastic upper bounds that is simpler to implement,
and will suffice for the example used in this post.
Like importance sampling, the estimator is parametrized by a number $N$ of particles to use.
The estimator obtains a single posterior sample $z_1 \sim p(\cdot | x)$ and
samples $N-1$ samples $z_i$ for $i = 2, \ldots, N$ from a proposal distribution $q(\cdot)$.
Then, it returns the average importance weight:

$$
\hat{p}(x) := \frac{1}{N} \sum_{i=1}^N \frac{p(z_i, x)}{q(z_i)}
$$

Note that this procedure is identical to the importance sampling estimator of the marginal likelihood,
except that one of the importance samples is sampled from the conditional distribution $p(\cdot | x)$ instead
of from the proposal distribution.
Like importance sampling, it is asymptotically consistent as $N \to \infty$, but
unlike importance sampling, it is a stochastic upper bound on the log marginal likelihood (for all $N$).

Aside: To see that $\mathbb{E}[1 / \hat{p}(x)] = 1/p(x)$ consider the following sampling distribution
on the vector $\mathbf{z} := (z_1, \ldots, z_N)$:

$$
q(\mathbf{z}) := \frac{1}{N} \sum_{i=1}^N p(z_i | x) \prod_{j \ne i} q(z_j)
$$

Then,

$$
\displaystyle \mathbb{E}_{\mathbf{z} \sim q(\cdot)} \left[ \displaystyle \frac{1}{\hat{p}(x)} \right]
=
\displaystyle \mathbb{E}_{\mathbf{z} \sim q(\cdot)} \left[ \displaystyle \frac{\prod_{i=1}^N q(z_i)}{\frac{1}{N} \sum_{i=1}^N p(z_i; x) \prod_{j \ne i} q(z_j)} \right] = \frac{1}{p(x)}
$$

The stochastic upper bound here, and those described in Grosse et al. 2015 all require one or more exact posterior samples.
While these can in principle be obtained using an exact inference method, like [rejection sampling](https://en.wikipedia.org/wiki/Rejection_sampling), in practice this is not feasible for large problems.
Instead, Grosse et al. observes that if we jointly simulate latent variables and observed variables $(z, x)$ from a generative model, then $z | x \sim p(\cdot | x)$, and we have a single exact conditional sample.

I applied this technique, using the importance sampling based stochastic lower and upper bounds described above.
For the stochastic lower bounds, I used an importance sampling estimate of the marginal likelhood based on the internal proposal of the model:

{% highlight julia %}
{% raw %}
(_, stoch_lb) = Gen.importance_resampling(heading_model, (), Gen.choicemap((:obs, obs)), num_particles)
{% endraw %}
{% endhighlight %}

For the stochastic upper bound, I implemented a procedure based on importance sampling described above, also using the internal proposal.
To return a stochastic upper bound, the procedure requires a trace with latent variables sampled from the conditional distribution given the observed variables.

{% highlight julia %}
{% raw %}
function stochastic_upper_bound(trace, observed::Selection, num_particles::Int)
    model = Gen.get_gen_fn(trace)
    model_args = Gen.get_args(trace)
    observations = Gen.get_selected(Gen.get_choices(trace), observed)
    log_weights = Vector{Float64}(undef, num_particles)
    log_weights[1] = Gen.project(trace, observed)
    for i=2:num_particles
        (_, log_weights[i]) = Gen.generate(model, model_args, observations)
    end
    log_total_weight = Gen.logsumexp(log_weights)
    log_ml_estimate = log_total_weight - log(num_particles)
    return log_ml_estimate
end
{% endraw %}
{% endhighlight %}

I jointly simulated 1000 times from the generative model.
For each simulated observed data $x$ (in this case, the observed angle `obs`), I ran black box variational inference, and plotted the resulting ELBO estimate after optimization in blue, with the angle `obs` on the x-axis.
For I also ran importance sampling with 1000 particles to obtain an stochastic lower bound estimate of the log marginal likelihood, shown in orange.
Finally, I input the single exact posterior sample $z$ and the observed data $x$ into the stochastic upper bound estimator described above with 1000 particles to obtain the stochastic upper bounds, shown in green.
The procedure for simulating a joint sample from the model and evaluating the stochastic lower and upper bounds is:

{% highlight julia %}
{% raw %}
function joint_sample_and_estimates(num_particles)
    sim_trace = Gen.simulate(heading_model, ())
    obs = sim_trace[:obs]
    (_, stoch_lb) = Gen.importance_resampling(heading_model, (), choicemap((:obs, obs)), num_particles)
    stoch_ub = stochastic_upper_bound(sim_trace, Gen.select(:obs), num_particles)
    return (stoch_lb, stoch_ub, obs)
end
{% endraw %}
{% endhighlight %}

The results are shown below.
The plot on the right shows the difference between the stochastic upper bounds and the ELBO estimates, for each simulated observation:

![combined-estimates]({{ page.assets }}/no-occlusion-exclusive-kls-1000.png){:class="img-responsive" width="900px"}
{: style="text-align: center;"}

First, note that the stochastic upper bounds and the stochastic lower bounds are generally very close,
suggesting that we have succesfully 'sandwiched' the log marginal likelihood (borrowing terminology from Grosse et al.).
However, technically the stochastic bounds are too loose to allow us to make confident quantitative statements based only on this evidence---they only guarantee that they sandwich the true log marginal likelihood within a few nats with high probability, and the span of the entire y-axis is on the order of a few nats as well.
The ability to estimate the log marginal likelihood to within a few nats is more significant in more difficult inference problems.
But, using the stochastic upper bound as our estimate of the log marginal likelhood, the resulting KL divergence estimates on the right seem to reflect the weakness in our variational approximating family.
The data suggest that for `obs` near 0 and $-\pi/2$ and $\pi/2$, the KL divergence is lower, which
agrees with our qualitative observations above.
For `obs` near $\pi$, our marginal likelihood estimators appear to have somewhat higher variance, and we are more uncertain about the KL divergence for those observations as a result.

I also ran same procedure but using stochastic lower and upper bounds using only 10 particles each, instead of 1000 particles:

![combined-estimates]({{ page.assets }}/no-occlusion-exclusive-kls-10.png){:class="img-responsive" width="900px"}
{: style="text-align: center;"}

Now, the gap between the stochastic lower and upper bounds is significant (note the scale of the y-axes in both plots).
In particular, whereas importance sampling fails to accurately estimate the log marginal likelihood,
the modified importance sampling procedure that uses the single posterior sample still gives a stochastic upper bound.
The estimates of the upper bound on the KL divergence are higher than the earlier estimates obtained using 1000 particles, and have more variance.
In particular, the pattern where the accuracy of the variational approximation is lower for obs near $\pi/4$ and $-\pi/4$ is no longer apparent.

## Conclusions

There are a few interesting conclusions that arise from this technique.

First, it is not surprising that an exact conditional sample is needed to make
any (even stochastic) guarantees about generic upper bounds on the log marginal likelihood.
The conditional distribution can assign arbitrary mass to arbitrarily small regions of the 
state space that are exceedingly unlikely to be examined by generic estimators.
The ability to sample from the conditional distribution ensures that these such regions have 
high probability of being sampled and therefore accounted for in the log marginal likelihood estimate.

Second, we can either run the technique for simulated data, or using a certifiably exact sampler,
like rejection sampling.
If we run the technique on simulated data only, then we do not get to choose which observations
we run this analysis for.
For example, in the above analysis, we have less data about the KL divergence in scenarios when the object lies behind
the robot, because these scenarios are less likely under our prior distribution.
It should be possible to fit regression models to the KL estimates shown above, that will allow us to be more confident about KL values in sparsely sampled regions of the observation space, and in order to interpolate and make predictions about the KL divergence for real-world data sets that were not simulated.

In a future post, I will describe a generalization of the technique in this post that I presented at NeurIPS 2017 called [AIDE](https://papers.nips.cc/paper/6893-aide-an-algorithm-for-measuring-the-accuracy-of-probabilistic-inference-algorithms.pdf), which
allows for many Monte Carlo algorithms can be evaluated using the same KL-divergence based metric used here and compared directly against variational approximations, as well as estimation of upper bounds on the symmetric KL divergence between two samplers.
I will also discuss how amortized variational inference and discriminative models can be interpreted from the perspective of approximation error of inference programs.

## References

[Rajesh Ranganath, Sean Gerrish, and David Blei. Black box variational inference. 2014.](http://proceedings.mlr.press/v33/ranganath14.html)

[Roger Grosse, Zoubin Ghahramani, and Ryan Adams. Sandwiching the marginal likelihood using bidirectional Monte Carlo. 2015.](https://arxiv.org/abs/1511.02543)

[Neal, Radford M. Annealed importance sampling. 2001.](https://link.springer.com/article/10.1023/A:1008923215028)

Christian Robert and George Casella. Monte Carlo statistical methods. 2013.

[Marco Cusumano-Towner and Vikash K. Mansinghka. AIDE: An algorithm for measuring the accuracy of probabilistic inference algorithms. 2017.](https://papers.nips.cc/paper/6893-aide-an-algorithm-for-measuring-the-accuracy-of-probabilistic-inference-algorithms.pdf)
